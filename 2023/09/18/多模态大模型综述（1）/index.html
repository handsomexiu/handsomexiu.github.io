<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"handsomexiu.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.17.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null,"show_result":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"giscus","storage":true,"lazyload":false,"nav":null,"activeClass":"giscus"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":"ture","trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="这篇文章是关于多模态大模型综述的一些介绍">
<meta property="og:type" content="article">
<meta property="og:title" content="多模态大模型综述（1）">
<meta property="og:url" content="https://handsomexiu.github.io/2023/09/18/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0%EF%BC%881%EF%BC%89/index.html">
<meta property="og:site_name" content="YNY&#39;s BLOG">
<meta property="og:description" content="这篇文章是关于多模态大模型综述的一些介绍">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://handsomexiu.github.io/2023/09/18/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0%EF%BC%881%EF%BC%89/image-20230918103651321.png">
<meta property="og:image" content="https://handsomexiu.github.io/2023/09/18/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0%EF%BC%881%EF%BC%89/image-20230918103851046.png">
<meta property="og:image" content="https://handsomexiu.github.io/2023/09/18/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0%EF%BC%881%EF%BC%89/image-20230918103805951.png">
<meta property="og:image" content="https://handsomexiu.github.io/2023/09/18/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0%EF%BC%881%EF%BC%89/image-20230918103925124.png">
<meta property="og:image" content="https://handsomexiu.github.io/2023/09/18/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0%EF%BC%881%EF%BC%89/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0%EF%BC%881%EF%BC%89%5Cimage-20230918104022770.png">
<meta property="og:image" content="https://handsomexiu.github.io/2023/09/18/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0%EF%BC%881%EF%BC%89/image-20230918104106070.png">
<meta property="og:image" content="https://handsomexiu.github.io/2023/09/18/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0%EF%BC%881%EF%BC%89/image-20230918104142519.png">
<meta property="article:published_time" content="2023-09-18T02:33:17.000Z">
<meta property="article:modified_time" content="2023-09-18T14:20:18.550Z">
<meta property="article:author" content="YNY">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://handsomexiu.github.io/2023/09/18/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0%EF%BC%881%EF%BC%89/image-20230918103651321.png">


<link rel="canonical" href="https://handsomexiu.github.io/2023/09/18/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0%EF%BC%881%EF%BC%89/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://handsomexiu.github.io/2023/09/18/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0%EF%BC%881%EF%BC%89/","path":"2023/09/18/多模态大模型综述（1）/","title":"多模态大模型综述（1）"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>多模态大模型综述（1） | YNY's BLOG</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">YNY's BLOG</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">YOLO</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%BC%E8%BF%B0a-survey-on-multimodal-large-language-models"><span class="nav-number">1.</span> <span class="nav-text"> 综述：A Survey on Multimodal Large Language Models</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">2.</span> <span class="nav-text"> 摘要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction"><span class="nav-number">3.</span> <span class="nav-text"> introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#overview"><span class="nav-number">4.</span> <span class="nav-text"> overview</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#method"><span class="nav-number">5.</span> <span class="nav-text"> method</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#multimodal-instruction-tuning"><span class="nav-number">5.1.</span> <span class="nav-text"> Multimodal Instruction tuning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#preliminaries"><span class="nav-number">5.1.1.</span> <span class="nav-text"> Preliminaries</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#modality-alignment"><span class="nav-number">5.1.2.</span> <span class="nav-text"> modality Alignment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE"><span class="nav-number">5.1.3.</span> <span class="nav-text"> 数据</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#benchmark-adaptation"><span class="nav-number">5.1.3.1.</span> <span class="nav-text"> benchmark adaptation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#self-instruction"><span class="nav-number">5.1.3.2.</span> <span class="nav-text"> Self-instruction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#hybrid-composition"><span class="nav-number">5.1.3.3.</span> <span class="nav-text"> Hybrid Composition</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%9D%E8%80%83"><span class="nav-number">5.1.3.4.</span> <span class="nav-text"> 思考</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#modality-bridging"><span class="nav-number">5.1.4.</span> <span class="nav-text"> Modality Bridging</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#evaluation-%E8%AF%84%E4%BC%B0"><span class="nav-number">5.1.5.</span> <span class="nav-text"> Evaluation 评估</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multimodal-in-context-learning"><span class="nav-number">5.2.</span> <span class="nav-text"> Multimodal In-Context learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multimodal-chain-of-thought"><span class="nav-number">5.3.</span> <span class="nav-text"> Multimodal Chain of thought</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#modality-bridging-2"><span class="nav-number">5.4.</span> <span class="nav-text"> Modality bridging</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#learning-paradigms"><span class="nav-number">5.5.</span> <span class="nav-text"> Learning paradigms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#llm-aided-visual-reasoningllm%E8%BE%85%E5%8A%A9%E8%A7%86%E8%A7%89%E6%8E%A8%E7%90%86"><span class="nav-number">5.6.</span> <span class="nav-text"> LLM-Aided Visual reasoning（LLM辅助视觉推理）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D"><span class="nav-number">5.6.1.</span> <span class="nav-text"> 介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#training-paradigms"><span class="nav-number">5.6.2.</span> <span class="nav-text"> Training Paradigms</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#functions"><span class="nav-number">5.6.3.</span> <span class="nav-text"> Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#evaluation"><span class="nav-number">5.6.4.</span> <span class="nav-text"> Evaluation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#challenges-and-future-directions"><span class="nav-number">6.</span> <span class="nav-text"> Challenges and Future Directions</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">7.</span> <span class="nav-text"> 总结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="YNY"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">YNY</p>
  <div class="site-description" itemprop="description">他强任他强，清风拂山岗</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/handsomexiu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;handsomexiu" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1184817330@qq.com" title="E-Mail → mailto:1184817330@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://handsomexiu.github.io/2023/09/18/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0%EF%BC%881%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="YNY">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YNY's BLOG">
      <meta itemprop="description" content="他强任他强，清风拂山岗">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="多模态大模型综述（1） | YNY's BLOG">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          多模态大模型综述（1）
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-09-18 10:33:17 / 修改时间：22:20:18" itemprop="dateCreated datePublished" datetime="2023-09-18T10:33:17+08:00">2023-09-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>15k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>14 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>这篇文章是关于多模态大模型综述的一些介绍</p>
<span id="more"></span>
<h1 id="综述a-survey-on-multimodal-large-language-models"><a class="markdownIt-Anchor" href="#综述a-survey-on-multimodal-large-language-models"></a> 综述：A Survey on Multimodal Large Language Models</h1>
<h1 id="摘要"><a class="markdownIt-Anchor" href="#摘要"></a> 摘要</h1>
<p>多模态大语言模型( Multimodal Large Language Model，MLLM )是近年来兴起的一个新的研究热点，它利用强大的大语言模型( Large Language Model，LLM )作为大脑来执行多模态任务。MLLM出人意料的突现能力（涌现的新能力），如基于图像编写故事和OCR - free数学推理，在传统方法中是罕见的，这表明了通向人工智能的潜在道路。本文旨在对MLLM的最新进展进行跟踪和总结。首先，我们给出了MLLM的提法（制定概念），并对其相关概念进行了阐述。然后，讨论了多模态指令调优( M-IT )、多模态上下文学习( M-ICL )、多模态思维链( M-CoT )和LLM辅助视觉推理( LAVR )等关键技术及其应用。最后，我们讨论现有的挑战</p>
<ul>
<li>OCR-free因该是指不许用转换成文本，直接处理数据，如这里直接处理公式，而不需要转换成文本。</li>
</ul>
<h1 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> introduction</h1>
<p>最近几年大语言模型得到了显著的进展。通过扩大数据规模和模型规模，这些LLMs具有了惊人的突现能力。</p>
<ul>
<li>尽管LLMs在大多数自然语言处理( Natural Language Processing，NLP )任务上表现出惊人的零/少样本推理性能，但由于它们只能理解离散文本，因此本质上对视觉&quot;瞎的&quot;。</li>
<li>同时大视觉基础模型在感知上发展很快，与文本的传统结合更注重<strong>模态对齐</strong>[ 11 ]和<strong>任务统一性</strong>[ 12 ]，在推理方面发展缓慢。<br>
鉴于这种互补性，单峰（单一的 unimodal，单模态）LLMs和视觉模型同时向对方运行（融合），最终导致了MLLM的新领域。<br>
从发展人工智能( Artificial General Intelligence，AGI )的角度来看，MLLM可能从LLM向前迈进了一步，原因如下：</li>
<li>( 1 ) MLLM更符合人类感知世界的方式。<strong>我们人类自然会接收到多感官的输入，这些输入往往是互补和合作的。</strong> 因此，多模态信息有望使MLLM更加智能化。</li>
<li>( 2 ) MLLM提供了更加友好的用户界面（接口）。得益于多模态输入的支持，用户可以更加灵活地与智能助手进行交互和交流。</li>
<li>( 3 ) Mllm是更全面的任务解决者。虽然LLMs通常可以执行NLP任务，但MLLMs通常可以支持更大范围的任务（多模态）<br>
GPT-4 [ 2 ]因为展示了令人惊叹的例子，引发了对MLLM的研究热潮。然而，GPT - 4并没有开放多模态界面（接口），迄今为止也没有公开该模型的任何信息。<br>
尽管如此，研究界为开发有能力的、开源的MLLM模型做出了许多努力；some surprising practical capabilities（实际应用能力） have been exhibited, such as writing website codes based on images [13], understanding the deep meaning of a meme [14], and OCR-free math reasoning [15].<br>
我们撰写此项综述，是为了让研究者对MLLM的基本思想、主要方法和当前进展有一个大致的把握。</li>
<li>需要注意的是，我们主要关注视觉和语言模态，也包括涉及其他模态的工作。</li>
<li>具体来说，我们将现有的MLLM划分为四种类型并进行相应的总结，同时打开一个GitHub页面进行实时更新。据我们所知，这是关于MLLM的第一次调查。</li>
</ul>
<h1 id="overview"><a class="markdownIt-Anchor" href="#overview"></a> overview</h1>
<p>本文将最近的代表性 MLLM 分为四种主要类型：多模态指令调整 (MIT)、多模态上下文学习 (M-ICL)、多模态思维链 (M-CoT) 和 LLM 辅助视觉推理 (LAVR，a general framework to build task-solving systems)。前三个构成了 MLLM 的基本原理，最后一个是一个以 LLM 作为核心的多模态系统。这三（难道不是四种？？，看本节的最后一句话，第四种相当于一个架构！）种技术相对独立，可以结合使用。</p>
<p>我们从 M-IT（第 3.1 节）的详细介绍开始，以揭示 LLM 如何从两个方面适应多模态：结构和数据。然后我们引入 M-ICL（第 3.2 节），这是一种在推理阶段常用的有效技术，以提高few shot性能（in-context learning<a href="%E7%BB%BC%E8%BF%B0%EF%BC%9AHarnessing%20the%20Power%20of%20LLMs%20in%20Practice%20%EF%BC%9AA%20Survey%20on%20ChatGPT%20and%20Beyond.md">综述：Harnessing the Power of LLMs in Practice ：A Survey on ChatGPT and Beyond</a>）。另一个重要的技术是 M-CoT (§3.3)，通常用于复杂的推理任务。之后，我们进一步总结了llm主要参与LAVR的几个角度（role，角色，LLM在LAVR中扮演的几个角色）(§3.4)，这通常涉及三种技术。最后，我们总结并给出潜在研究方向。</p>
<h1 id="method"><a class="markdownIt-Anchor" href="#method"></a> method</h1>
<h2 id="multimodal-instruction-tuning"><a class="markdownIt-Anchor" href="#multimodal-instruction-tuning"></a> Multimodal Instruction tuning</h2>
<p>instruction是指对任务的描述。</p>
<ul>
<li>Instruction Tuning（指令调优）是一种涉及在指令格式数据集[16]集合上微调预训练的llm的技术。通过这种方式进行调整，LLM 可以通过遵循新指令泛化（扩展）到看不见的任务，从而提高零样本性能。这种简单而有效的想法引发了后续工作在 NLP 领域的成功，例如 ChatGPT [1]、InstructGPT [17]、FLAN [16, 18] 和 OPT-IML [19]。<br>
<img src="image-20230918103651321.png" alt="image-20230918103651321"></li>
<li>有监督的微调方法通常需要许多特定任务的数据来训练特定任务的模型。（pretrain-finetune）</li>
<li>提示方法减少了对大规模数据的依赖，可以通过提示工程完成专门的任务。（prompt engineering）
<ul>
<li>few-shot性能能够提高</li>
<li>但是zero-shot的性能相当平均。对zero-shot的效果一般</li>
<li>虽然小样本性能得到了提高，但零样本性能仍然相当平均[ 5 ]。</li>
</ul>
</li>
<li>不同的是，指令微调学习如何泛化到看不见的任务，而不是像two counterparts那样拟合特定的任务（two counterparts：前面的哪两种方法）。
<ul>
<li>instruction tuning is highly related to multi-task prompting [20].</li>
</ul>
</li>
</ul>
<p>传统的多模态模型仍然局限于前两种tuning范式，缺乏zero shot（零样本）能力。<br>
因此，最近的许多工作[ 13、21、22]探索了将LLMs中的教学调整成功扩展到多模态。<strong>为了从单模态扩展到多模态，数据和模型都需要进行相应的适应</strong>。</p>
<ul>
<li>对于数据，研究人员通常通过改编已有（现有的）的基准数据集[23 - 28]或通过self-instruction获取M-IT数据集[13,21,29]。</li>
<li>对于模型：常见的方法就是将多模态数据注入LLMs中，利用大模型的强力的推理能力分析其他模态的数据</li>
<li>相关工作或者直接将外来嵌入（其余模态的嵌入，输入的特征化）对齐到LLMs [ 21、23 ~ 25、27、28、30 ~ 32]中，或者借助专家模型将外来（其余）模态翻译成LLMs可以摄取（处理）的自然语言[ 33、34]。</li>
<li>通过这种方式，<strong>这些工作通过多模态指令调优将LLM转化为多模态聊天机器人</strong>[ 13、21、22、33、35]和多模态通用任务求解器[ 23、24、26]。</li>
</ul>
<p>在本节的后面部分，我们首先给出基础知识( § 3.1 . 2 )。在过渡到M - IT的定义之前，我们额外引入了M - IT之前的一个常见过程，即<strong>对齐预训练</strong>（alignment pre-training）( § 3.1.3 )。接下来，我们将剩余内容整理为图2所示：首先介绍M - IT数据是如何收集的( § 3.1.4 )，然后详细讨论MLLMs的模型适配，即不同模态之间的各种弥（bridge the gap）合方式( § 3.1.5 )。最后，我们介绍了评估指令调整MLLM的评估方法( § 3.1.6 )。<br>
<img src="image-20230918103851046.png" alt="image-20230918103851046"></p>
<h3 id="preliminaries"><a class="markdownIt-Anchor" href="#preliminaries"></a> Preliminaries</h3>
<p>本部分简要说明了多模态指令样本的一般结构和M - IT的一般流程。<br>
<img src="image-20230918103805951.png" alt="image-20230918103805951"></p>
<ul>
<li>多模态指令样本通常包括一条指令和一个输入输出对
<ul>
<li>该指令通常是描述任务的自然语言句子，例如，“详细描述图像”。</li>
<li>输入可以是类似于视觉问答( VQA )任务的图像-文本对[ 46 ]，也可以是类似于图像描述任务的图像[ 47 ]。</li>
<li>输出是对以输入为条件的指令的回答。</li>
</ul>
</li>
<li>如表1所示，指令模板是灵活的，且受手工设计[ 21、31、33]的约束。值得注意的是，指令样本也可以泛化为多轮（multi-round）指令，其中多模态输入共享[ 21、30、31、43]。</li>
<li>在形式上，多模态指令样本可以表示为三元组形式。$$(I,M,R)$$其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo separator="true">,</mo><mi>M</mi><mo separator="true">,</mo><mi>R</mi></mrow><annotation encoding="application/x-tex">I,M,R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span>表示：instruction，the multimodal input ,the ground truth response</li>
<li>MLLM在给定指令和多模态输入的情况下预测一个答案$$A=f(I,M;\theta)$$</li>
<li>这里A表示预测的answer，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>表示模型的参数。训练目标是通常用于训练LLMs [ 21、30、32、43]的原始自回归目标（auto-regressive objective），在此基础上，MLLM被要求预测回答（response）的下一个token。目标为：$$\mathcal{L}(\theta)=-\sum\limits_{i=1}^{N}\log p(R_{i}|I,R_{&lt;i};\theta)$$其中N是真实response（回答）的回答</li>
</ul>
<h3 id="modality-alignment"><a class="markdownIt-Anchor" href="#modality-alignment"></a> modality Alignment</h3>
<p>在论文<a href="%E7%BB%BC%E8%BF%B0%EF%BC%9AHarnessing%20the%20Power%20of%20LLMs%20in%20Practice%20%EF%BC%9AA%20Survey%20on%20ChatGPT%20and%20Beyond.md">综述：Harnessing the Power of LLMs in Practice ：A Survey on ChatGPT and Beyond</a>中有提到<code>human alignment</code>，增加了鲁棒性<br>
通常对成对数据（pair-data ）进行大规模的(相对于指令整定)预训练，<strong>以鼓励不同模态[ 25,29,35,38]之间的对齐</strong>，这在M - IT之前。对齐数据集通常为图像-文本对[ 48-56 ]或自动语音识别( Automatic Speech Recognition，ASR ) [ 57-59 ]数据集，<strong>均包含文本</strong>。更具体地说，图像-文本对以自然语言句子的形式描述图像，而ASR数据集包含语音的转录。<strong>对齐预训练的常用方法是将预训练好的模块(如视觉编码器、LLM等)冻结，并训练一个可学习的接口[ 21,37,38]。</strong></p>
<h3 id="数据"><a class="markdownIt-Anchor" href="#数据"></a> 数据</h3>
<p><strong><mark>从数据集看来，我认为这也是一种finetuning，但是这里的数据集是可以构造的，以动态的适应不同的任务</mark></strong><br>
多模态指令数据的采集是M - IT的关键。收集方法大致可分为以下几类：</p>
<ul>
<li>benchmark adaptation</li>
<li>self-instruction</li>
<li>hybrid composition</li>
</ul>
<h4 id="benchmark-adaptation"><a class="markdownIt-Anchor" href="#benchmark-adaptation"></a> benchmark adaptation</h4>
<p>benchmark数据集是高质量数据的丰富来源。<br>
大量的研究[23 - 26,28,29,32,35]直接利用了现有的基准数据集来构造指令格式的数据集。以VQA数据集的转换为例，原始样本是一个输入输出对，其中<strong>输入包括图像和自然语言问题（&lt;image&gt; {question}）</strong>，输出是基于图像的问题的文本答案。这些数据集的输入-输出对可以自然地包含指令样本的多模态输入和回答。指令，即任务的描述，<strong>可以来自手动设计，也可以来自GPT辅助下的半自动生成</strong>。具体来说，一些作品[13,23,25,26,36,37]手工制作了一个候选指令pool（池，这里还不是池化，相当于存储器），并在训练期间在池中进行采样。我们为VQA数据集提供了一个指令模板示例，如表2所示。<strong>另一些工作则手工设计一些种子指令，并使用这些指令提示GPT生成更多的种子指令[24,31,33]。</strong><br>
<img src="image-20230918103925124.png" alt="image-20230918103925124"><br>
但是由于现有VQA和caption数据集的答案通常很简洁，直接使用这些数据集进行指令调优可能会限制MLLM的输出长度。解决这个问题有两种常见的策略：</p>
<ul>
<li>第一个是修改instruction，直接告诉LLM我们要简短、单句。例如，ChatBridge[29]明确声明short和brief用于简答数据，以及a snetence和single sentence用于caption数据。类似地，InstructBLIP[23]将short 和brief插入到公共数据集的指令模板中，这些公共数据集的回答更倾向于简短的。
<ul>
<li>既然无法变长，那我就显示的直接修改instruction，添加short，brief的内容，这样显示的学习，可以让模型直接学习short与response（答案）的关系，避免的一些隐式的学习所造成的影响。</li>
</ul>
</li>
<li>第二种方法是延长现有答案的长度[36]。例如，M3IT[36]提出通过用原始问题、答案和上下文提示ChatGPT来改写原始答案。</li>
</ul>
<h4 id="self-instruction"><a class="markdownIt-Anchor" href="#self-instruction"></a> Self-instruction</h4>
<p>尽管现有的基准数据集可以提供丰富的数据源，但它们通常不能很好地满足现实场景中的人类需求，例如<strong>多轮对话（multiple round）</strong>。为了解决这个问题，一些作品通过自我指导（self-instruction）来收集样本[60]，这引导（booststrap）llm使用一些手工注释的样本来生成遵循文本指令（textual instruction-following）的数据。<strong>具体来说，一些遵循指令（instruction-following）的样本是手工制作的种子（seed）样本，然后提示ChatGPT/GPT-4以种子样本为指导生成更多的指令样本。</strong>(和benchmark中的最后面的一样都是GPT-Aid，GPT辅助生成)<s>（<code>这个和上面的区别在哪，benchmark那，这里应该和benchmark那的，这里的更加针对任务，一些benchmark无法满足的特定任务</code>）</s> LLaVA[21]通过将图像翻译成带有caption和边界框的文本，并提示GPT-4在种子示例的上下文中生成新数据，将该方法扩展到多模态领域。通过这种方式，构建了一个M-IT数据集，称为llava - instruction -150k。根据这一思路，随后的作品如MiniGPT-4[13]、ChatBridge[29]、GPT4Tools[34]和DetGPT[38]开发了不同的M-IT数据集，以满足不同的需求。<br>
（<code>这里可以看一下table 3</code>）<br>
<s>- instruction样本和instruction-following样本是指什么<br>
- instruction-following样本因该是基于instruction构造的，需要生成出来的instruction满足要求。</s></p>
<ul>
<li>instruction-following 样本是指遵循instruction的样本（从input到output）
<ul>
<li>对于这里所说的可能是根据自己设计的一些指令，然后已经注释好的样本作为seed，输入到GPT4中作为上下文，再这个环境下，再根据对图片的编码：解析成文本text（）内容和位置，然后再相应的instructions，输入到GPT4中去，得到的response与输入和instructions构成新的样本：instruction-following。</li>
<li><strong>query：查询，询问</strong></li>
</ul>
</li>
</ul>
<h4 id="hybrid-composition"><a class="markdownIt-Anchor" href="#hybrid-composition"></a> Hybrid Composition</h4>
<p>Apart from the M-IT data, languageonly user-assistant conversation data can also be used to improve conversational proficiencies and instruction-following abilities [22, 31, 32, 35].（除去上面的MIT数据，纯文本的数据也可以用于提高对话的熟练度和模型遵循指令的能力。）<br>
<strong>Multi Instruct [ 26</strong> ]探讨了融合单模态和多模态数据的不同训练策略，包括混合指令调优(混合两种数据并随机打乱)、顺序指令调优(文本数据紧接着是多模态数据)和基于Adapter的顺序指令调优。实证结果表明，在多模态数据上，混合指令调优至少不比单独调优差。</p>
<h4 id="思考"><a class="markdownIt-Anchor" href="#思考"></a> 思考</h4>
<p>如何构造instruct-follow 数据或prompt数据集还得继续思考</p>
<h3 id="modality-bridging"><a class="markdownIt-Anchor" href="#modality-bridging"></a> Modality Bridging</h3>
<p>由于LLMs只能感知文本，因此弥合自然语言和其他模态之间的鸿沟是必要的。然而，以端到端的方式训练一个大型的多模态模型是很昂贵的。而且，这样做会带来灾难性遗忘的风险[ 61 ]。</p>
<ul>
<li>因此，一种更实用的方法是在预训练的视觉编码器和LLM之间引入一个可学习的接口。（Learnable Interface）
<ul>
<li>可学习的接口负责在<strong>冻结预训练模型的参数</strong>时连接不同的模态。(<code>交替式训练？？</code>)</li>
<li>挑战在于如何高效地将视觉内容翻译成LLM能够理解的文本。
<ul>
<li>A common and feasible solution is to leverage <strong>a group of learnable query tokens</strong> to extract information in a query-based manner [62], which first has been implemented in Flamingo [63] and BLIP-2 [64], and subsequently inherited by a variety of work [23,25,42].</li>
<li>Furthermore, some methods use a <strong>projection-based</strong> （基于投影）interface to close the modality gap [21, 30, 38, 43].</li>
<li>adapter：也有一些工作探索了一种参数有效的调优方式。LLaMA- Adapter [ 28、35]在训练时在Transformer中引入了一个轻量级的适配器模块。La VIN [ 32 ]设计了一个混合模态适配器来动态决定多模态嵌入的权重。</li>
</ul>
</li>
</ul>
</li>
<li>另一种方法是借助专家模型将图像翻译成语言，然后将语言发送给LLM。（expert model）
<ul>
<li>除了learnable interface，使用专家模型，如图像描述模型，也是弥合模态鸿沟的可行方法[ 35 ]。</li>
<li>与之不同的是，专家模型背后的思想是在没有训练的情况下将多模态输入转换为语言。
<ul>
<li>这样，语言学习者就可以通过转换后的语言间接地理解多模态。</li>
<li>例如，videochat-Text [ 33 ]使用预训练的视觉模型来提取动作等视觉信息，并使用语音识别模型来丰富描述。</li>
<li>尽管使用专家模型是直接的，但它可能不像采用Learnable interface那样灵活。多（其余）模态（foreign modalities）转换为文本通常会造成信息损失。正如videochat- Text [ 33 ]指出的那样，将视频转化为文本描述会扭曲时空关系。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="evaluation-评估"><a class="markdownIt-Anchor" href="#evaluation-评估"></a> Evaluation 评估</h3>
<p>评价M - IT后模型性能的指标有很多，根据问题类型（genres）可以大致分为两类，包括closed-set和open- set。<br>
<strong>Closed-set</strong></p>
<ul>
<li>封闭式问题指的是一种可能的答案选项已预先确定并限制在一个有限集合内的问题。评估通常在benchmark-adapted数据集上进行。
<ul>
<li>在这种情况下，response自然可以通过benchmark指标来判断 [21, 23, 25, 26, 28, 29, 32, 35]。</li>
<li>The evaluation settings are typically zero-shot [23,26,29,36] or finetuning [21,23,25,28,32,35–37].
<ul>
<li>The first setting often selects a wide range of datasets covering different general tasks and splits them into <code>held-in and held-out </code>datasets. After tuning on the former, <strong>zero-shot</strong> performance is evaluated on the latter with unseen datasets or even unseen tasks. 会尽可能涵盖更广泛的一般任务</li>
<li>the second setting is often observed in the evaluation of domain-specific downstream tasks.(<code>finetuning</code>)主要做的是特定领域下的学习</li>
</ul>
</li>
<li>上述评估方法通常仅限于小范围的选定任务或数据集，缺乏全面的定量比较。
<ul>
<li>为此，一些人努力开发专为 MLLM 设计的新基准 [39, 40, 72]。
<ul>
<li>例如，Fu 等人[73] 构建了一个综合评估基准 MME，其中包括总共 14 项感知和认知任务。MME 中的所有指令-答案对都是人工设计的，以避免数据泄露。</li>
<li>LAMM-Benchmark [39] 的提出是为了在各种二维/三维视觉任务中对 MLLM 进行定量评估。</li>
<li>Video-ChatGPT [40] 为基于视频的会话模型提出了一个定量评估框架，其中包含两类评估，即基于视频的生成性能评估和 zeroshot 问答评估。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Open-set</strong>
<ul>
<li>与封闭式问题相比，开放式问题的回答可以更加灵活，<strong>MLLM 通常在其中扮演聊天机器人的角色</strong>。<strong>由于聊天内容可以是任意的</strong>，因此与封闭式输出相比，判断起来更加棘手。</li>
<li>标准可分为人工评分、GPT 评分和案例研究。
<ul>
<li>manual：人工评分需要人工对生成的回答进行评估。这种方法通常涉及手工设计的问题，旨在评估特定的维度。
<ul>
<li>例如mPLUG-Owl [22]收集了一个与视觉相关的评估集合，用于评估自然图像理解、图表和流程图理解等能力。类似地，GPT4Tools [34]构建了两个集合，分别用于微调和zero-shot性能评估，并从思想、行动、论点和整体方面评估回答。</li>
</ul>
</li>
<li>GPT：由于人工评估耗费大量人力，一些研究人员探索了使用 GPT 进行评分，即 GPT 评分。
<ul>
<li>这种方法通常用于评估多模态对话的性能。
<ul>
<li>LLaVA [21]提议通过GPT-4对回答进行评分，评估其在帮助性和准确性等方面的表现。具体而言，从COCO [48]验证集中随机选择30个图像，每个图像都有一个简短的问题、一个详细的问题和一个复杂的推理问题，通过在GPT-4上进行自我训练，产生由MLLM和GPT-4生成的答案，并将它们发送给GPT-4进行比较。</li>
</ul>
</li>
<li>基于 GPT-4 的评分的一个主要问题是，目前其多模态界面尚未公开。因此，GPT-4 只能根据与图像相关的文本内容（如标题或边界框坐标）生成响应，而无法访问图像[37]。因此，在这种情况下，将 GPT-4 设置为性能上限可能是有问题的。</li>
<li>另一种方法是通过案例研究比较MLLM的不同能力。例如，mPLUG-Owl使用一个与视觉相关的笑话理解案例来与GPT-4 [2]和MM-REACT [14]进行比较。类似地，Video-LLaMA [42]提供了一些案例，展示了音频-视觉共感知和常识概念识别等能力。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>other</strong>
<ul>
<li>还有一些方法侧重于 MLLM 的特定方面。例如，MultiInstruct [26] 提出了一种称为灵敏度的指标，用于评估模型对不同指令的鲁棒性。</li>
<li><strong><mark>Li 等人[44] 深入研究了对象幻觉问题，并提出了一种查询方法 POPE 来评估这方面的性能。</mark></strong></li>
<li><mark><code>Zhao 等人[45]考虑了安全问题，并提出评估 MLLM 对对抗性攻击的鲁棒性。</code></mark></li>
</ul>
</li>
</ul>
<h2 id="multimodal-in-context-learning"><a class="markdownIt-Anchor" href="#multimodal-in-context-learning"></a> Multimodal In-Context learning</h2>
<p>ICL是LLMs重要的新（emergent 涌现）能力之一。<br>
ICL具有两个优点：</p>
<ul>
<li>（1）与传统的监督学习范式不同，传统的监督学习范式是从大量数据中学习隐含的模式，ICL的关键在于从类比（analogy）中学习[74]。
<ul>
<li>具体的，在ICL的设置中，具体来说，在 ICL 环境中，LLMs从几个例子和一个可选指令中学习，并推断出新的问题，从而以少样本的方式解决复杂且未知的任务[14, 75, 76]。</li>
</ul>
</li>
<li>（2）ICL通常以无需训练的方式[74]实施，在推理阶段可以灵活地集成到不同的框架中。
<ul>
<li>与 ICL 密切相关的一项技术是指令调整（instruction tuning）（见第 3.1 节），经验表明它能增强 ICL 的能力[16]。</li>
</ul>
</li>
</ul>
<p>在 MLLM 的背景下，ICL 被扩展到更多模态，从而产生了多模态 ICL（M-ICL）。基于（<strong>§3.1.2</strong>）中的设置，在推理时，M-ICL 可以通过在原始样本中添加演示集（即上下文样本集，a demonstration set，如a set of in-context samples ）来实现。在这种情况下，可以扩展模板，如表 3 所示。需要注意的是，我们列出了两个上下文中的示例以作说明，但示例的数量和排序可以灵活调整。事实上，模型通常对演示的安排很敏感[74, 77]。<br>
<img src="%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0%EF%BC%881%EF%BC%89%5Cimage-20230918104022770.png" alt="image-20230918104022770"><br>
在多模态应用方面，M-ICL主要用于两个场景：</p>
<ul>
<li>解决各种视觉推理任务 [14, 27, 63, 78, 79]
<ul>
<li>前者通常涉及从一些（a-few）特定任务的例子中学习，然后归纳出新的但类似的问题。</li>
<li>通过指令和演示所提供的信息，LLM能够了解任务在做什么，输出模板是什么，并最终生成预期的答案。（上下文学习）（和M-IT紧密结合）</li>
</ul>
</li>
<li>教导LLM使用外部工具[75, 76, 80]。
<ul>
<li>工具使用的示例通常是纯文本的，而且更加精细（细粒度）。它们通常由一连串可按顺序执行的步骤组成，以完成任务。因此，第二种情况与 CoT 密切相关（见第 3.3 节）。</li>
</ul>
</li>
</ul>
<h2 id="multimodal-chain-of-thought"><a class="markdownIt-Anchor" href="#multimodal-chain-of-thought"></a> Multimodal Chain of thought</h2>
<p>正如开创性（pioneer，先前的）的工作[7]所指出的那样，思维链是“一系列中间推理步骤”，已经被证明在复杂推理任务中是有效的[7, 87, 88]。思维链的主要思想是提示LLM不仅输出最终答案，还输出导致答案的推理过程，类似于人类的认知过程</p>
<p>受自然语言处理领域的成功启发，已经提出了多个工作来将单模态思维链扩展到多模态思维链（M-CoT）。我们在图3中总结了这些工作。首先，类似于M-IT中的情况（参见第3.1节），需要填补模态差距（gap，bridge）（第3.3.1节）。然后，我们引入了不同的范式来获得M-CoT能力（第3.3.2节）。最后，我们描述了M-CoT的更具体方面，包括链的配置（第3.3.3节）和链的形式（第3.3.4节）。<br>
<img src="image-20230918104106070.png" alt="image-20230918104106070"></p>
<h2 id="modality-bridging-2"><a class="markdownIt-Anchor" href="#modality-bridging-2"></a> Modality bridging</h2>
<p>要将 NLP 的成功经验应用于多模态，首先要解决的问题就是模态桥接。实现这一目标的方法大致有两种：融合特征或将视觉输入转化为文本描述。与第 3.1.5 节中的情况类似，我们将它们分别归类为可学习接口和专家模型，并依次进行讨论。</p>
<ul>
<li><strong>可学习接口</strong>这种方法涉及采用可学习接口将视觉嵌入映射到词嵌入空间。然后，将映射后的嵌入作为prompt发送给LLM，与其他语言一起引发M-CoT推理。例如，CoT-PT [81]将多个Meta-Net链接起来进行prompt微调，以模拟推理链，其中每个Meta-Net将视觉特征嵌入到与提示（prompt）相关的特定步骤偏置中（<code>where each Meta-Net embeds visual features into a step-specific bias to the prompt，其中，每个Meta - Net将视觉特征嵌入到对提示的特定步骤的偏见中</code>）。Multimodal-CoT [82]采用了一个两阶段的框架，具有共享的基于Transformer的结构[89]，其中视觉和文本特征通过交叉注意力进行交互。</li>
<li><strong>专家模型</strong>：引入一个专家模型将视觉输入转换为文本描述是一种可行的模态连接方法。例如，ScienceQA [65]采用图像字幕模型，并将图像字幕和原始语言输入的拼接传递给LLM。尽管简单直接，这种方法在字幕过程中可能会造成信息丢失[33, 82]</li>
</ul>
<h2 id="learning-paradigms"><a class="markdownIt-Anchor" href="#learning-paradigms"></a> Learning paradigms</h2>
<p>学习范式也是一个值得研究的方面。获得M - CoT能力的途径大致有三种</p>
<ul>
<li>finetuning</li>
<li>training-free few/zero shot</li>
<li>三种方式对样本量的要求是依次递减的</li>
</ul>
<p><strong>finetuning</strong>：直观上，<strong>微调方法往往涉及到对特定数据集进行M - CoT学习。</strong> 例如，Science QA [ 65 ]构建了一个带有讲座和解释的科学问答数据集，该数据集可以作为学习CoT推理的来源，并在该数据集上进行微调。多模态CoT [ 82 ]也使用了Science QA基准，但<strong>以两步方式</strong>生成输出，即理据(推理步骤链)和基于理据的最终答案。CoT-PT [ 81 ]通过结合即时调整和特定步骤的视觉偏向来学习一个隐式的推理链。<br>
<strong>few/zero-shot</strong>：小样本/零样本学习在计算效率上更高效。</p>
<ul>
<li>它们之间的主要区别在于，小样本学习通常需要手工制作一些（a few（这里可以理解成少量的））语境（in-context）样本，以便模型能够更容易地一步一步地学习推理。</li>
<li>零样本学习不需要任何具体的CoT学习实例。（将指令instruction作为prompt）
<ul>
<li>在这种情况下，通过提示设计的指令（如“让我们逐帧思考”或“这两个关键帧之间发生了什么”）（In this case, by prompting designed instructions like “Let’s think frame by frame” or “What happened between these two keyframes”）[85, 86]，模型学习利用嵌入的知识和推理能力，无需明确的指导。类似地，一些工作[14, 83]使用任务和工具使用的描述作为提示（prompt），<strong>将复杂任务分解为子任务</strong>。<br>
<strong>Chain Configuration（链配置）</strong><br>
链的配置是推理的一个重要方面，可以分为自适应和预定义形式（<code>链式配置是推理的一个重要方面，可分为自适应配置和预定义配置。</code>）。自适应配置要求LLM自行决定何时停止推理链[14, 65, 75, 76, 82, 83]，而预定义配置使用预定义的长度停止链[81, 84–86]。<br>
<strong>Generation Patterns 生成模式</strong><br>
链是如何构建的是一个值得研究的问题。我们将当前的工作总结为：</li>
</ul>
</li>
<li>基于填充的模式（an infillingbased pattern）：具体来说，基于填充的模式要求通过前后上下文（前后步骤）之间的推理步骤推断来填补逻辑间隙[85, 86]。</li>
<li>基于预测的模式（a predicting-based pattern）：基于预测的模式要求根据指令和以前的推理历史[14, 65, 75, 76, 82, 83]来扩展推理链。</li>
<li><strong>这两种类型的模式都要求生成的步骤应保持一致和正确</strong></li>
</ul>
<h2 id="llm-aided-visual-reasoningllm辅助视觉推理"><a class="markdownIt-Anchor" href="#llm-aided-visual-reasoningllm辅助视觉推理"></a> LLM-Aided Visual reasoning（LLM辅助视觉推理）</h2>
<h3 id="介绍"><a class="markdownIt-Anchor" href="#介绍"></a> 介绍</h3>
<p>受工具增强型LLMs成功的启发[ 95-98 ]，一些研究探讨了调用外部工具[ 14、34、75、76]或视觉基础模型[ 14,83,84,91,92,99]进行视觉推理任务的可能性。这些工作以LLMs作为不同角色的帮助者，构建了任务特定（<strong><mark>task-specific</mark></strong>）的[ 84、90、93 ]或通用的（<strong><mark>general-purpose</mark></strong>）[ 14,75,76,80,83]视觉推理系统。</p>
<p>与传统的视觉推理模型[100–102]相比，这些工作具有几个优点：</p>
<ul>
<li><strong>较强的泛化能力。</strong> 利用从大规模预训练中学习到的丰富的开放世界知识，这些系统可以很容易地泛化到未见过的对象或概念，并具有显著的零/小样本性能[ 75、76、90、91、93、94]。</li>
<li><strong>Emergent abilities</strong> ：借助于LLMs强大的推理能力和丰富的知识，这些系统能够完成复杂的任务。例如，给定一幅图像，MM-REACT [ 14 ]可以解释表面下的含义，例如解释meme为何是有趣的。</li>
<li><strong>Better interactivity and control 更好的互动性和控制性</strong>
<ul>
<li>更好的交互性和控制性。传统模型通常允许一组有限的控制机制，并且往往需要昂贵的精简（组织好的，curated）数据集[ 103、104 ]。</li>
<li>相比之下，基于LLM的系统具有在用户友好界面中进行精细控制的能力(例如点击和自然语言查询)[ 84 ]。<br>
本节的以下部分按照图4中的显示顺序组织：首先介绍在构建LLM辅助视觉推理系统时采用的不同训练范式（第3.4.2节）。随后，我们深入探讨LLM在这些系统中扮演的主要角色（第3.4.3节）。最后，我们以各种类型的性能评估总结我们的讨论。<br>
<img src="image-20230918104142519.png" alt="image-20230918104142519"></li>
</ul>
</li>
</ul>
<h3 id="training-paradigms"><a class="markdownIt-Anchor" href="#training-paradigms"></a> Training Paradigms</h3>
<p>根据训练范式的不同，LLM辅助的视觉推理系统可以分为training-free和微调finetuning两类。<br>
<strong>training-free</strong></p>
<ul>
<li>在预训练的LLMs中存储了丰富的先验知识，一种直观而简单的方法是冻结预训练模型，直接促使LLMs满足各种需求。根据设定，推理系统可以进一步分为少样本模型和零样本模型。
<ul>
<li>少样本模型（few-shot）[ 14、75、76、80 ]需要一些手工设计的语境（in-context）样本(<strong>见§ 3.2</strong> )来指导LLMs生成程序（programs）或执行步骤序列。这些程序或执行步骤作为相应基础模型或外部工具/模块的指令。</li>
<li>零样本模型则更进一步，直接利用LLM的语言学/语义学知识或推理能力。例如，PointCLIP V2 [ 93 ]促使GPT - 3生成具有3D相关语义的描述，以便更好地与相应的图像对齐。在CAT [ 84 ]中，LLMs被指导根据用户查询对字幕进行精化。<br>
<strong>Finetuning</strong></li>
</ul>
</li>
<li>为了激活工具使用方面的规划能力并提高系统的指令跟随（<strong>instruction-following</strong>）能力，GPT4Tools [ 34 ]引入了指令调整方法(<strong>参见§ 3.1</strong> )。收集了一个新的工具相关指令数据集，并使用该数据集对模型进行微调。</li>
</ul>
<h3 id="functions"><a class="markdownIt-Anchor" href="#functions"></a> Functions</h3>
<p>为了进一步考察LLM在LLM辅助的视觉推理系统中究竟扮演何种角色，现有的相关工作分为三类</p>
<ul>
<li>LLM as a Controller</li>
<li>LLM as a Decision Maker</li>
<li>LLM as a Semantics Refiner<br>
前两个角色，即控制者和决策者，与CoT有关(见§ 3.3 )。它经常被使用，因为复杂的任务需要分解成中间更简单的步骤。单轮任务中Controller 更常见，多轮任务中Decision Maker更常见。<br>
<strong>LLM as a Controller</strong></li>
<li>在这种情况下，LLMs充当中央控制器
<ul>
<li>将一个复杂的任务分解为更简单的子任务/步骤
<ul>
<li>第一步通常是利用LLMs的CoT能力来完成的。第二步将这些任务分配给合适的工具/模块。具体来说，llm被明确提示输出任务规划[80]，或者更直接地输出要调用的模块[34,75,76]。例如，VISPROG[76]提示GPT-3输出一个可视化程序，其中每个程序行调用一个模块来执行一个子任务。此外，LLMs还需要为模块输入输出参数名。为了处理这些复杂的需求，一些手工制作的语境(见§ 3.1 )例子被用作参考[ 75、76、80 ]。这与推理链的优化(见§3.3)密切相关，或者更具体地说，是least-to-most prompting[105]技术。通过这种方式，复杂问题被分解成子问题依次解决。<br>
<strong>LLM as a Decision Maker</strong></li>
</ul>
</li>
</ul>
</li>
<li>在这种情况下，复杂任务以多轮的方式进行求解，往往以迭代的方式进行[ 91 ]。决策者通常履行以下职责：( 1 )总结当前上下文和历史信息，判断当前步骤可获得的信息是否足以回答问题或完成任务；( 2 )整理和归纳答案，以用户友好的方式呈现。<br>
<strong>LLM as a Semantics Refiner</strong></li>
<li>当LLM作为语义提炼者时，研究人员主要利用他们丰富的语言学和语义学知识。具体来说，LLM经常被要求将信息整合到连贯流畅的自然语言句子中[94]，或者根据不同的特定需求生成文本[84,90,93]</li>
</ul>
<h3 id="evaluation"><a class="markdownIt-Anchor" href="#evaluation"></a> Evaluation</h3>
<p>有两种方法可以评估LLM-Aided视觉推理系统的性能，即基准度量[75,76,90,91,93,94]和手动评估[34,76,92]。手动人工评估一般都是需要对模型的某些特殊方面进行评估，例如生成结果的丰富性、准确性，或者模型生成的某种思想等。（<code>有点像M-IT中的那一块</code>）</p>
<h1 id="challenges-and-future-directions"><a class="markdownIt-Anchor" href="#challenges-and-future-directions"></a> Challenges and Future Directions</h1>
<p>MLLM的发展还处于初级阶段，仍有很大的改进空间，现总结如下：</p>
<ul>
<li>目前的多层感知机在感知能力上仍然有限，导致视觉信息获取不完全或错误。这可能是由于信息容量和计算负担之间的折中。更具体地说，Q-Former [ 64 ]仅使用32个可学习的标记（tokens）来表示一幅图像，这可能会导致信息丢失。尽管如此，增大token大小势必会给输入长度通常有限的LLMs带来较大的计算负担。一种潜在的方法是引入像SAM [ 8 ]这样的大型视觉基础模型来更有效地压缩视觉信息[ 21、29 ]。</li>
<li>MLLM的推理链条可能是脆弱的。例如，Fu等[ 73 ]发现，在一个数学计算案例中，MLLM虽然计算出了正确的结果，但由于推理的断裂，仍然给出了错误的答案。这表明单模态LLM的推理能力可能不等于LLM接收到视觉信息后的推理能力。如何改进多模态推理是一个值得研究的课题。</li>
<li>MLLMs的指令跟随能力需要提升。M - IT之后，尽管有明确的指令&quot;请回答是或否&quot; [ 73 ]，但仍有部分MLLM无法生成预期的答案( ‘是’或’否’)。这表明，指令调优可能需要覆盖更多的任务以提高泛化性</li>
<li>object<strong>幻觉</strong>问题是广泛存在的[ 13、44]，这在很大程度上影响了MLLMs的可靠性。这可能是由于对齐预训练不足造成的[ 13 ]。因此，<strong>一种可能的解决方案是在视觉和文本模态之间进行更细粒度的对齐</strong>。<code>细粒度是指图像的局部特征，可以通过SAM [ 21、29 ]获取，以及相应的局部文本描述。</code></li>
<li>需要Parameter-efficient training。现有的两种模态桥接方式，即可学习接口和专家模型，都是为减少计算负担而进行的初步探索。更有效的训练方法可以在有限的计算资源下释放更多的功能。</li>
</ul>
<h1 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h1>
<p>在本文中，我们对现有的MLLM文献进行了调查，并对其主要方向进行了概述，包括三种常见的技术( M - IT、M - ICL、MCoT)和构建任务解决系统的通用框架( LAVR )。此外，我们还强调了当前有待填补的研究空白，并指出了一些有前景的研究方向。我们希望此次调查能够让读者对当前MLLM的进展有一个清晰的认识，并激发更多的工作。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/LLM/" rel="tag"># LLM</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/09/18/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0%EF%BC%881%EF%BC%89/" rel="prev" title="大模型综述（1）">
                  <i class="fa fa-chevron-left"></i> 大模型综述（1）
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/10/11/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="next" title="自动驾驶相关论文阅读">
                  自动驾驶相关论文阅读 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  
  <div class="comments giscus-container">
  </div>
  
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YNY</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">116k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:45</span>
  </span>
</div>



    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css" integrity="sha256-hLTCMFlKxdNgPXyWlSSxYN0ykJmxxq9Yt3MNfdRGWeA=" crossorigin="anonymous">



  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #4D4D4C;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #F7F7F7;
      background-image: linear-gradient(#F7F7F7, #F7F7F7);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>

  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
          if(result)$(this).text('复制成功')
          else $(this).text('复制失败')
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('复制')
        }, 300)
      }).append(e)
    })
  </script>

<script class="next-config" data-name="giscus" type="application/json">{"enable":true,"repo":"handsomexiu/comments","repo_id":"R_kgDOJzfTTQ","category":"Announcements","category_id":"DIC_kwDOJzfTTc4CXcBf","mapping":"pathname","data_strict":0,"reactions_enabled":1,"emit_metadata":0,"theme":"light","lang":"zh-CN","crossorigin":"anonymous","input_position":"bottom"}</script>

<script>
document.addEventListener('page:loaded', () => {
  if (!CONFIG.page.comments) return;

  NexT.utils.loadComments('.giscus-container')
    .then(() => NexT.utils.getScript('https://giscus.app/client.js', {
      attributes: {
        async                   : true,
        crossOrigin             : 'anonymous',
        'data-repo'             : CONFIG.giscus.repo,
        'data-repo-id'          : CONFIG.giscus.repo_id,
        'data-category'         : CONFIG.giscus.category,
        'data-category-id'      : CONFIG.giscus.category_id,
        'data-mapping'          : CONFIG.giscus.mapping,
        'data-reactions-enabled': CONFIG.giscus.reactions_enabled,
        'data-emit-metadata'    : CONFIG.giscus.emit_metadata,
        'data-theme'            : CONFIG.giscus.theme,
        'data-lang'             : CONFIG.giscus.lang,
        'data-input-position'   : CONFIG.giscus.input_position,
        'data-loading'          : CONFIG.giscus.loading
      },
      parentNode: document.querySelector('.giscus-container')
    }));
});
</script>

</body>
</html>
