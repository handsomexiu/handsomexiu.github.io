<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>GAT和GCN中的注意事项</title>
    <url>/2023/08/24/GAT%E5%92%8CGCN%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</url>
    <content><![CDATA[<h1 id="inductive-learning-and-transductive-learning"><a class="markdownIt-Anchor" href="#inductive-learning-and-transductive-learning"></a> “Inductive learning” and “Transductive learning”</h1>
<p>“Inductive learning”意为归纳学习，“Transductive learning”意为直推学习</p>
<p>对于GCN而言我们认为其是：直推学习，也就是说当测试集出现了训练集未学习过的节点时即图结构发生了变化时，网络需要重新训练。</p>
<p>对于GAT而言：归纳学习；也就是训练阶段见不到的数据（在图书剧中可以指新的节点，也可以指新的图）                                                                                                                                        直接进行预测而不需要重新训练。</p>
<span id="more"></span>
<p>GCN就像是没有权重的GAT一样，见如下公式：</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>G</mi><mi>C</mi><mi>N</mi><mo>=</mo><mover accent="true"><mi>A</mi><mo>~</mo></mover><mi>X</mi><mi>W</mi><mspace linebreak="newline"></mspace><mi>G</mi><mi>A</mi><mi>T</mi><mo>=</mo><mo stretchy="false">(</mo><mover accent="true"><mi>A</mi><mo>~</mo></mover><mo>⊙</mo><mi>M</mi><mo stretchy="false">)</mo><mi>X</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">GCN=\tilde{A}XW \\
GAT=(\tilde A \odot M)XW
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">G</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9201899999999998em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9201899999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">A</span></span></span><span style="top:-3.6023300000000003em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.11110999999999999em;"><span class="mord">~</span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">G</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1701899999999998em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9201899999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">A</span></span><span style="top:-3.6023300000000003em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.11110999999999999em;"><span class="mord">~</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span></span></p>
<p>这里的需不需要重新训练围殴认为是其关注的重点，对于GCN而言重点关注的<strong>图的全局结构</strong>，因此当图的结果变换的时候自然需要重新训练。</p>
<p>而对于GAT而言虽说用到了邻接矩阵，但训练的目标是<mark>中心节点</mark>和<mark>邻居节点</mark>间的聚合操作。</p>
<p>某种意义上来说，GCN是一种考虑了整体图结构的方法；而GAT一定程度上放弃了整体结构，这使得其能够完成Inductive任务。<br>
链接：<a href="https://www.zhihu.com/question/409415383/answer/1361505060">https://www.zhihu.com/question/409415383/answer/1361505060</a></p>
<p>其实是否确保inductive，本质上在于两点：首先是你要确保你这个算法的node-level input不能是one hot而必须是实在的node attribute，一旦onehot了就必是只能transductive，原因显然。其次是training方式，不能是依赖于整图的矩阵运算，而必须是graphsage里面appendix a的minibatch training模式下的分割方案，而这才是graphsage有底气说自己inductive牛逼的主要原因。你确保这两点，几乎现在市面上所有message passing架构的gnn都是inductive的。<br>
链接：<a href="https://www.zhihu.com/question/409415383/answer/1361596817">https://www.zhihu.com/question/409415383/answer/1361596817</a></p>
<p>这个地方还可以参考论文：<a href="https://www.researchgate.net/publication/352513259_A_Subgraph-based_Knowledge_Reasoning_Method_for_Collective_Fraud_Detection_in_E-commerce">https://www.researchgate.net/publication/352513259_A_Subgraph-based_Knowledge_Reasoning_Method_for_Collective_Fraud_Detection_in_E-commerce</a></p>
<p>里面提到了了一个<strong>全局和局部</strong>的观念</p>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GNN</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>QCNext:何为下一代？</title>
    <url>/2023/09/02/QCNext-%E4%BD%95%E4%B8%BA%E4%B8%8B%E4%B8%80%E4%BB%A3%EF%BC%9F/</url>
    <content><![CDATA[<p>这是一篇关于轨迹预测的文章，文章主要介绍了轨迹预测的论文《QCNeXt: A Next-Generation Framework For Joint Multi-Agent Trajectory Prediction》。文章对论文进行了翻译和分析，并就下一代进行了讨论。笔者论文阅读量不够，遇到错误还麻烦指出，十分感谢！</p>
<span id="more"></span>
<h1 id="文章内容"><a class="markdownIt-Anchor" href="#文章内容"></a> 文章内容</h1>
<p>QCnext是QCnet的扩展。</p>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> abstract</h2>
<p>估计道路智能体未来轨迹的联合分布对于自动驾驶至关重要。在这份技术报告中，论文提出了一种称为 QCNeXt 的下一代联合多智能体轨迹预测框架。</p>
<ul>
<li>首先，论文采用以query为中心的编码范式来完成联合多智能体轨迹预测的任务。在这种编码方案的支持下，论文的场景编码器配备了集合元素上的排列等变性、空间维度上的旋转平移不变性以及时间维度上的平移不变性。这些不变性不仅从根本上实现了准确的多智能体预测，而且还赋予编码器流处理的能力。</li>
<li>其次，论文提出了一种 DETR -like的多智能体解码器，它通过对未来时间步长的智能体交互建模来促进联合多智能体轨迹预测。</li>
<li>论文首次证明，即使在边际指标上，联合预测模型也可以优于边际预测模型，这为轨迹预测开辟了新的研究机会。</li>
<li>论文的方法在 Argoverse 2 多智能体运动预测基准上排名第一，赢得了 CVPR 2023 自动驾驶研讨会上 Argoverse 挑战赛的冠军。</li>
</ul>
<h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> introduction</h2>
<p>轨迹预测是自动驾驶中最棘手的问题之一。为了实现安全的自动驾驶，轨迹预测模型必须准确预测自动驾驶车辆周围的一个或多个目标主体（例如车辆、行人、骑自行车的人等）的未来运动。</p>
<p>在多智能体预测方面，大多数先前的工作主要集中于估计目标智能体未来轨迹的<strong>边际分布</strong>，<mark><strong>即假设多个智能体的未来运动是条件独立的。</strong></mark></p>
<ul>
<li>在多智能体预测方面，大多数先前的工作主要集中于估计目标智能体未来轨迹的边际分布，即假设多个智能体的未来运动是条件独立的。</li>
<li><strong>对于这里我的理解是</strong>：对于多智能体的联合预测没有考虑未来的交互，直接输出每个智能体的结果。就好比对场景和过去的轨迹交互进行编码之后，直接送给解码器进行轨迹的解码，而解码器是没有考虑未来的交互的；这种情况和有一种情况类似：单智能体的预测，这是TNT，mmtransformer的做法，这些方法假设了每个智能体都可以用这种方法进行预测自己未来的轨迹。</li>
</ul>
<p>其他一些论文则通过考虑未来的社交互动来关注联合多智能体轨迹预测。然而，这些方法都无法达到与边际指标的边际预测模型相同的性能水平。人们认为联合预测任务比边际预测任务困难得多。</p>
<p>在这份技术报告中，论文提出了一种用于联合多智能体轨迹预测的下一代建模框架，它可以准确地估计多个目标智能体的联合未来分布。论文将该框架称为 QCNeXt，因为它是 QCNet 的下一代，<strong>QCNet 是世界上最强大的边缘轨迹预测模型之一</strong>。 QCNeXt 采用基于 Transformer 的编码器解码器架构作为其前身。</p>
<ul>
<li>对于编码器，论文继承了 HiVT [6] 和 QCNet [5] 的对称设计，使模型具有集合元素上的排列等变性、空间维度上的旋转平移不变性和时间维度上的平移不变性。这些不变性属性有助于模型实现准确的多智能体预测并实现流场景编码。</li>
<li>对于解码器，论文扩展了 QCNet 的解码流程来联合预测变体，<strong>它可以明确地捕获智能体在未来时间步骤的社交互动。</strong></li>
<li>此外，论文引入了场景评分模块来估计所有目标智能体联合未来轨迹的可能性。</li>
<li>再 Argoverse 2 多智能体运动预测基准 [4] 上的实验表明 QCNeXt 可以准确预测场景级别的轨迹。作为联合预测模型，QCNeXt 即使在边际指标上也能优于 QCNet，这展示了论文解决方案的有效性。</li>
</ul>
<h2 id="approach"><a class="markdownIt-Anchor" href="#approach"></a> approach</h2>
<h3 id="query-centric-scene-encoder"><a class="markdownIt-Anchor" href="#query-centric-scene-encoder"></a> Query-Centric Scene Encoder</h3>
<p>论文的场景编码器与 QCNet [5] 中使用的相同，它是一种基于分解注意力的 Transformer，可以捕获时间依赖性、智能体映射交互和社交交互。编码器的整体架构如图1所示。</p>
<img src="/.com//QCNext-何为下一代？\image-20230902165946533.png" alt="image-20230902165946533" style="zoom: 80%;">
<p>论文采用 QCNet 中以查询为中心的范例来对场景元素进行编码。这种编码范式背后的哲学是相对时空，它指导论文为模型配备空间维度的旋转平移不变性和时间维度的平移不变性。在该范式中，为每个场景元素建立局部时空坐标系，包括车道、人行横道、车辆、行人等。然后将这些场景元素编码在其局部坐标系中以产生不变的表示，场景元素之间的关系为Transformers 在相对时空位置嵌入的帮助下捕获。具体来说，在执行 QKV 注意力之前，注意力层中的键/值元素与相对于查询元素的时空位置嵌入相连接。在地图-地图注意力以及一系列时间注意力、智能体-地图注意力和社会注意力之后，场景编码器产生形状 [M, D] 的地图编码和形状 [A, T, D] 的智能体编码，其中 M 、 A 、 T 、 D 分别是地图多边形、建模智能体、历史时间步和隐藏单元的数量。这些编码稍后将用作解码器中的场景上下文。</p>
<h3 id="multi-agent-detr-decoder"><a class="markdownIt-Anchor" href="#multi-agent-detr-decoder"></a> Multi-Agent DETR Decoder</h3>
<p>论文的解码流程遵循 QCNet 解码器的设计选择，其中循环、无锚点轨迹提议模块以数据驱动的方式生成自适应轨迹锚点，然后是基于锚点的轨迹细化模块，用于预测轨迹锚点的偏移。然而，QCNet 的原始解码器没有考虑未来时间步长的智能体之间的社交互动，因为它只聚合当前时间步长的相邻智能体的编码。因此，QCNet 解码器仅适用于边缘轨迹预测。为了解决这个问题，论文提**出了一种新的类似 DETR 的解码器，可以捕获未来的社交互动。**论文的解码器的详细架构如图 2 所示。</p>
<p><img src="/.com//.%5CQCNext-%E4%BD%95%E4%B8%BA%E4%B8%8B%E4%B8%80%E4%BB%A3%EF%BC%9F%5Cimage-20230902190544810.png" alt="image-20230902190544810"></p>
<p>在训练之前，论文用 D(通道数) 的大小随机初始化 K 个嵌入。然后将每个嵌入重复 A 次（应该是指agent的数量）以形成形状为 [K, A, D] 的tensor，其中每一行用作 K 个联合 future 之一的初始种子。</p>
<ul>
<li>
<p>对于该tensor的每一行，A 嵌入首先使用 Mode2Time 交叉注意模块进行更新，这使得每个嵌入负责场景中一个智能体的预测。</p>
</li>
<li>
<p>然后，Mode2Map 交叉注意模块用相邻地图信息更新嵌入。</p>
</li>
<li>
<p>接下来，论文在嵌入tensor上应用行自注意力，其目的是对每个联合场景智能体之间的社交互动进行建模</p>
</li>
</ul>
<p>这三个模块交错堆叠 Ldec 次，后面是一个列式自注意力模块，使 K 个联合场景能够相互通信。然后使用 MLP （这里其实也有一个假设，就是未来的时间步都是独立的，这样就不用像循环神经网络一样每一个时间步去执行了（当然MLP也可能潜在的学习了轨迹之间的联系））从更新的嵌入tensor中解码 2 秒的轨迹。为了预测接下来 2 秒的轨迹，论文让更新后的嵌入tensor再次成为 Mode2Time 交叉注意力模块的输入（也就是说在循环模块中只有第一次输入时anchor，后面的是更新后的tensor），并重复上述过程。<strong>这个计算过程会循环进行</strong>，直到完成 6 秒的轨迹。</p>
<p>[K,A,T,4]：k代表每个智能体预测的轨迹数量（多模态输出，单模态输出会导致不因该出现多模态轨迹平均的结果，且不符合实际情况[3]）；A：智能体数量；T未来的时间步；4：每一个时间步输出的结果数量</p>
<p>proposal模块预测的轨迹充当refinement模块的锚点。与proposal模块相比，refinement模块的初始嵌入张量是不可学习的（这里指的是初始的anchor是随机确定的），而是从proposal模块输出的轨迹中推导出来的(这里的proposal是由anchor更新过来的)。其余的架构proposal模块的架构类似，不同之处在于 MLP 预测器在单个镜头中输出轨迹锚点的偏移量，而不使用任何循环机制。（这一块的内容有点像mmtransformer中的stack transformer）</p>
<h3 id="scene-scoring-module"><a class="markdownIt-Anchor" href="#scene-scoring-module"></a> Scene Scoring Module</h3>
<p>与 QCNet 的解码器通过应用于后细化模式嵌入的 MLP 生成智能体级轨迹分数相比，<strong>论文的解码器需要生成场景级置信度分数以适应联合轨迹预测任务</strong>。<mark>场景评分模块将场景中所有目标智能体的后细化模式嵌入作为输入</mark>。为了为每个联合预测生成一个置信度得分，论文需要一些场景级池化算子将所有目标智能体的模式嵌入总结为一个场景嵌入，并通过 MLP 从中解码置信度得分。典型的池化方法包括平均池化、最大池化、注意力池化等。论文凭经验选择注意力池化，因为论文注意到一些目标智能体有不感兴趣的行为（例如，保持静态），不应该对场景分数的计算贡献太大。（一些不该出现，不切实际的场景因该赋予低分）</p>
<h1 id="关于下一代的讨论"><a class="markdownIt-Anchor" href="#关于下一代的讨论"></a> 关于“下一代的讨论”</h1>
<p>论文中所提到的“下一代”是指 QCNet 的下一代。</p>
<p>论文就此更广泛的讨论一下这一点：</p>
<p>从过去到如今轨迹预测模型经历了三个阶段：基于物理的方法，基于maneuver的方法，基于交互感知的方法。</p>
<p>其中基于物理的方法根据车辆的运动学和动力学特征预测车辆的未来轨迹。这些方法忽略了其他车辆和基础设施对目标车辆运动的影响，因此它们经常在超过一秒的预测范围内失败。如过去的一些跟车模型IDM模型，GIPPS模型等。</p>
<p>基于机动的方法根据一组机动原型（maneuver prototype）预测目标车辆的未来运动。这些将道路的结构考虑在内进行长期预测，但仍然忽略了车辆间的交互作用。</p>
<p>基于交互感知的方法，将驾驶作为一种交互活动，吸引了越来越多的兴趣，与那些非交互感知方法相比表现出更好的性能。他们中的大多数只考虑车辆之间的相互作用而忽略了交通基础设施。</p>
<p>现在的方法基本都属于交互感知的方法：如TNT，Dense TNT，HOMO，QCNet等方法。</p>
<p>再交互感知的方法中也出现了一些的分支，我们就输出结果的方式进行探讨：</p>
<p>就输出结果而言：分为单模态输出，多模态输出。单智能体输出，多智能体输出，并行输出，循环输出</p>
<ul>
<li>单模态输出是模型只输出一条轨迹，该方法存在一定的缺陷，在论文[3]中通过实验发现该输出结果会是多模态输出的一种平均形式，会造成与实际不符合的情况</li>
<li>多模态输出是一种比较火也是比较复合实际的输出方式，这样的输出模拟的人的不确定性驾驶行为，模仿人类的驾驶意图，且效果更好。</li>
<li>单智能体输出是指模型只输出一个智能体的结果，并且该模型可以在所有智能体上使用，这样可以节省计算成本，降低耗时；也可以看成是每辆车在未来没有交互，和被当成是边缘分布一样。</li>
<li>多智能体输出
<ul>
<li>边缘分布的方法：每个智能体类似于单独输出的，关于未来不存在交互</li>
<li>联合分布的方法：每个智能体在未来是有交互的，不独立。这样可以一定程度上保证一点的安全性，避免不安全的轨迹出现
<ul>
<li>除了本文所采用的方法之外（隐式的对安全性进行建模，因为联合分布中包括后面的评分，对轨迹冲突的分数会很低甚至不会出现对应的情况）；</li>
<li>另一种可以参考UniAD这篇论文，利用了未来估计的occupancy网格来使得驾驶更加安全。</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>并行输出是指像MLP或TNT这样（MLP可能隐含学习了未来时间步之间的一种表示；对于TNT这种可能在对anchor修正的时候也潜在的学习了未来时间步之间的联系）这样直接输出几个时间步内的结果，这存在一个潜在的假设（在给定anchor的情况下（TNT，multipath））未来的时间步是独立的；这样的计算效率更高</li>
<li>循环输出类似于social-LSTM这样，一个一个时间步的输出，这样的效率比较低，之间学习了未来时间步之间的联系。</li>
</ul>
<h1 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h1>
<p>[1]Zhou Z, Wen Z, Wang J, et al. QCNeXt: A Next-Generation Framework For Joint Multi-Agent Trajectory Prediction[J]. arXiv preprint arXiv:2306.10508, 2023.<a href="https://arxiv.org/abs/2306.10508">https://arxiv.org/abs/2306.10508</a></p>
<p>[2]Mo X, Xing Y, Lv C. Recog: A deep learning framework with heterogeneous graph for interaction-aware trajectory prediction[J]. arXiv preprint arXiv:2012.05032, 2020.<a href="https://arxiv.org/abs/2012.05032">https://arxiv.org/abs/2012.05032</a></p>
<p>[3]Cui H, Radosavljevic V, Chou F C, et al. Multimodal trajectory predictions for autonomous driving using deep convolutional networks[C]//2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019: 2090-2096.<a href="https://arxiv.org/abs/1809.10732">https://arxiv.org/abs/1809.10732</a></p>
]]></content>
      <categories>
        <category>轨迹预测</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自动驾驶</tag>
        <tag>轨迹预测</tag>
      </tags>
  </entry>
  <entry>
    <title>First-Hello World</title>
    <url>/2023/06/24/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="quick-start"><a class="markdownIt-Anchor" href="#quick-start"></a> Quick Start</h2>
<h3 id="create-a-new-post"><a class="markdownIt-Anchor" href="#create-a-new-post"></a> Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server"><a class="markdownIt-Anchor" href="#run-server"></a> Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files"><a class="markdownIt-Anchor" href="#generate-static-files"></a> Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites"><a class="markdownIt-Anchor" href="#deploy-to-remote-sites"></a> Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
<h3 id="github同步"><a class="markdownIt-Anchor" href="#github同步"></a> GitHub同步</h3>
<p>hexo clean</p>
<p>hexo d -g</p>
<p>会有一点延迟，更新得等一会</p>
<h3 id="显示部分内容"><a class="markdownIt-Anchor" href="#显示部分内容"></a> 显示部分内容</h3>
<p>在你写 md 文章的时候，可以在内容中加上 <code>&lt;!--more--&gt;</code>，这样首页和列表页展示的文章内容就是 <code>&lt;!--more--&gt;</code> 之前的文字，而之后的就不会显示了。</p>
]]></content>
      <categories>
        <category>软件使用</category>
      </categories>
  </entry>
  <entry>
    <title>pytorch_data&amp;detach</title>
    <url>/2023/08/24/pytorch-data-detach/</url>
    <content><![CDATA[<p>这是关于pytorch中的.data操和detach()操作的区分和介绍</p>
<p>这两个方法都可以用来从原有的计算图中分离出某一个tensor，有相似的地方，也有不同的地方，下面来比较性的看一看</p>
<p>原文链接：<a href="https://blog.csdn.net/qq_27825451/article/details/96837905">https://blog.csdn.net/qq_27825451/article/details/96837905</a></p>
<span id="more"></span>
<h1 id="data"><a class="markdownIt-Anchor" href="#data"></a> data</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"> </span><br><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3.</span>], requires_grad = <span class="literal">True</span>)</span><br><span class="line">out = a.sigmoid()</span><br><span class="line">c = out.data  <span class="comment"># 需要走注意的是，通过.data “分离”得到的的变量会和原来的变量共用同样的数据，而且新分离得到的张量是不可求导的，c发生了变化，原来的张量也会发生变化</span></span><br><span class="line">c.zero_()     <span class="comment"># 改变c的值，原来的out也会改变</span></span><br><span class="line"><span class="built_in">print</span>(c.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(out.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;----------------------------------------------&quot;</span>)</span><br><span class="line"> </span><br><span class="line">out.<span class="built_in">sum</span>().backward() <span class="comment"># 对原来的out求导，</span></span><br><span class="line"><span class="built_in">print</span>(a.grad)  <span class="comment"># 不会报错，但是结果却并不正确</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;运行结果为：</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">tensor([0., 0., 0.])</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">tensor([0., 0., 0.], grad_fn=&lt;SigmoidBackward&gt;)</span></span><br><span class="line"><span class="string">----------------------------------------------</span></span><br><span class="line"><span class="string">tensor([0., 0., 0.])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>（1）tensor .data 返回和 x 的相同数据 tensor,而且这个新的tensor和原来的tensor是共用数据的，一者改变，另一者也会跟着改变，而且新分离得到的tensor的require s_grad = False, 即不可求导的。（这一点其实detach是一样的）</p>
<p>（2）使用tensor.data的局限性。文档中说使用tensor.data是不安全的, 因为 <mark><strong>x.data 不能被 autograd 追踪求微分</strong></mark> 。什么意思呢？从上面的例子可以看出，**由于我更改分离之后的变量值c,导致原来的张量out的值也跟着改变了，但是这种改变对于autograd是没有察觉的，它依然按照求导规则来求导，导致得出完全错误的导数值却浑然不知。**它的风险性就是如果我再任意一个地方更改了某一个张量，求导的时候也没有通知我已经在某处更改了，导致得出的导数值完全不正确，故而风险大。</p>
<p>(也就是说.data修改数据后不会被检测到，但是原始操作已经修改)</p>
<h1 id="detach"><a class="markdownIt-Anchor" href="#detach"></a> detach()</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"> </span><br><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3.</span>], requires_grad = <span class="literal">True</span>)</span><br><span class="line">out = a.sigmoid()</span><br><span class="line">c = out.detach()  <span class="comment"># 需要走注意的是，通过.detach() “分离”得到的的变量会和原来的变量共用同样的数据，而且新分离得到的张量是不可求导的，c发生了变化，原来的张量也会发生变化</span></span><br><span class="line">c.zero_()     <span class="comment"># 改变c的值，原来的out也会改变</span></span><br><span class="line"><span class="built_in">print</span>(c.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(out.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;----------------------------------------------&quot;</span>)</span><br><span class="line"> </span><br><span class="line">out.<span class="built_in">sum</span>().backward() <span class="comment"># 对原来的out求导，</span></span><br><span class="line"><span class="built_in">print</span>(a.grad)  <span class="comment"># 此时会报错，错误结果参考下面,显示梯度计算所需要的张量已经被“原位操作inplace”所更改了。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">tensor([0., 0., 0.])</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">tensor([0., 0., 0.], grad_fn=&lt;SigmoidBackward&gt;)</span></span><br><span class="line"><span class="string">----------------------------------------------</span></span><br><span class="line"><span class="string">RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>tensor.detach()的两点总结：</p>
<p>（1）tensor .detach() 返回和 x 的相同数据 tensor,而且这个新的tensor和原来的tensor是共用数据的，一者改变，另一者也会跟着改变，而且新分离得到的tensor的require s_grad = False, 即不可求导的。（这一点其实 .data是一样的）（也是在原数据集上操作）</p>
<p>（2）使用tensor.detach()的优点。从上面的例子可以看出，由于我更改分离之后的变量值c,导致原来的张量out的值也跟着改变了，这个时候如果依然按照求导规则来求导，由于out已经更改了，所以不会再继续求导了，而是报错，这样就避免了得出完全牛头不对马嘴的求导结果。</p>
<h1 id="区别总结"><a class="markdownIt-Anchor" href="#区别总结"></a> 区别总结</h1>
<p>相同点：tensor.data和tensor.detach() 都是变量从图中分离，但而这都是“原位操作 inplace operation”。</p>
<p>不同点：</p>
<p>（1）.data 是一个属性，二.detach()是一个方法；</p>
<p>（2）.data 是不安全的，.detach()是安全的。</p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>计算机语言</tag>
      </tags>
  </entry>
  <entry>
    <title>用户新增预测挑战赛</title>
    <url>/2023/08/24/%E7%94%A8%E6%88%B7%E6%96%B0%E5%A2%9E%E9%A2%84%E6%B5%8B%E6%8C%91%E6%88%98%E8%B5%9B/</url>
    <content><![CDATA[<h2 id="1数据说明"><a class="markdownIt-Anchor" href="#1数据说明"></a> 1.数据说明</h2>
<p>赛题数据由约62万条训练集、20万条测试集数据组成，共包含13个字段。其中uuid为样本唯一标识，eid为访问行为ID，udmap为行为属性，其中的key1到key9表示不同的行为属性，如项目名、项目id等相关字段，common_ts为应用访问记录发生时间（毫秒时间戳），其余字段x1至x8为用户相关的属性，为匿名处理字段。target字段为预测目标，即是否为新增用户。</p>
<h2 id="2评估指标"><a class="markdownIt-Anchor" href="#2评估指标"></a> 2.评估指标</h2>
<p>本次竞赛的评价标准采用f1_score，分数越高，效果越好。</p>
<span id="more"></span>
<h2 id="3解题思路"><a class="markdownIt-Anchor" href="#3解题思路"></a> 3.解题思路</h2>
<p>参赛选手的任务是基于训练集的样本数据，构建一个模型来预测测试集中用户的新增情况。这是一个二分类任务，其中目标是根据用户的行为、属性以及访问时间等特征，预测该用户是否属于新增用户。具体来说，选手需要利用给定的数据集进行特征工程、模型选择和训练，然后使用训练好的模型对测试集中的用户进行预测，并生成相应的预测结果。</p>
<h2 id="4遇到的问题"><a class="markdownIt-Anchor" href="#4遇到的问题"></a> 4.遇到的问题</h2>
<ul>
<li>数据量比较大，但是特征比较少，经过处理的特征没几个，因此目的是先增加特征然后再对特征进行处理以及特征降维</li>
<li>还不知道数据集的具体情况，可以对数据集进行筛选（暂时还没进行）</li>
</ul>
<h2 id="5方案"><a class="markdownIt-Anchor" href="#5方案"></a> 5.方案</h2>
<h3 id="相关模块和数据的导入"><a class="markdownIt-Anchor" href="#相关模块和数据的导入"></a> 相关模块和数据的导入：</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="comment">#简单来说LabelEncoder就是把n个类别值编码为0~n-1之间的整数，建立起1-1映射</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMClassifier</span><br><span class="line"><span class="comment">#load() missing 1 required positional argument: &#x27;Loader&#x27;</span></span><br><span class="line"><span class="comment">#E:\software\anaconda\anaconda3\Lib\site-packages\distributed\config.py文件里的</span></span><br><span class="line"><span class="comment">#yaml.load(f)改成yaml.safe_load(f)</span></span><br><span class="line"><span class="keyword">from</span> catboost <span class="keyword">import</span> CatBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> HistGradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> StackingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取训练集和测试集</span></span><br><span class="line"><span class="comment"># 使用 read_csv() 函数从文件中读取训练集数据，文件名为 &#x27;train.csv&#x27;</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">&#x27;用户新增预测挑战赛公开数据/train.csv&#x27;</span>)</span><br><span class="line"><span class="comment"># 使用 read_csv() 函数从文件中读取测试集数据，文件名为 &#x27;test.csv&#x27;</span></span><br><span class="line">test_data = pd.read_csv(<span class="string">&#x27;用户新增预测挑战赛公开数据/test.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train_data<span class="comment">#用于观察数据集</span></span><br></pre></td></tr></table></figure>
<h3 id="udmap的处理将-字典中的数据和unknown数据以one-hot的存储"><a class="markdownIt-Anchor" href="#udmap的处理将-字典中的数据和unknown数据以one-hot的存储"></a> udmap的处理，将 字典中的数据和unknown数据以one-hot的存储</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3. 将 &#x27;udmap&#x27; 列进行 One-Hot 编码 </span></span><br><span class="line"><span class="comment"># 数据样例：</span></span><br><span class="line"><span class="comment">#                    udmap  key1  key2  key3  key4  key5  key6  key7  key8  key9</span></span><br><span class="line"><span class="comment"># 0           &#123;&#x27;key1&#x27;: 2&#125;     2     0     0     0     0     0     0     0     0</span></span><br><span class="line"><span class="comment"># 1           &#123;&#x27;key2&#x27;: 1&#125;     0     1     0     0     0     0     0     0     0</span></span><br><span class="line"><span class="comment"># 2  &#123;&#x27;key1&#x27;: 3, &#x27;key2&#x27;: 2&#125;   3     2     0     0     0     0     0     0     0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 python 中, 形如 &#123;&#x27;key1&#x27;: 3, &#x27;key2&#x27;: 2&#125; 格式的为字典类型对象, 通过key-value键值对的方式存储</span></span><br><span class="line"><span class="comment"># 而在本数据集中, udmap实际是以字符的形式存储, 所以处理时需要先用eval 函数将&#x27;udmap&#x27; 解析为字典</span></span><br><span class="line"><span class="comment"># 具体实现代码：</span></span><br><span class="line"><span class="comment"># 定义函数 udmap_onethot，用于将 &#x27;udmap&#x27; 列进行 One-Hot 编码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">udmap_onethot</span>(<span class="params">d</span>):</span><br><span class="line">    v = np.zeros(<span class="number">9</span>)  <span class="comment"># 创建一个长度为 9 的零数组</span></span><br><span class="line">    <span class="keyword">if</span> d == <span class="string">&#x27;unknown&#x27;</span>:  <span class="comment"># 如果 &#x27;udmap&#x27; 的值是 &#x27;unknown&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> v  <span class="comment"># 返回零数组</span></span><br><span class="line">    d = <span class="built_in">eval</span>(d)  <span class="comment"># 将 &#x27;udmap&#x27; 的值解析为一个字典</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10</span>):  <span class="comment"># 遍历 &#x27;key1&#x27; 到 &#x27;key9&#x27;, 注意, 这里不包括10本身</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;key&#x27;</span> + <span class="built_in">str</span>(i) <span class="keyword">in</span> d:  <span class="comment"># 如果当前键存在于字典中</span></span><br><span class="line">            v[i-<span class="number">1</span>] = d[<span class="string">&#x27;key&#x27;</span> + <span class="built_in">str</span>(i)]  <span class="comment"># 将字典中的值存储在对应的索引位置上</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> v  <span class="comment"># 返回 One-Hot 编码后的数组</span></span><br></pre></td></tr></table></figure>
<h3 id="数据集特征提取"><a class="markdownIt-Anchor" href="#数据集特征提取"></a> 数据集特征提取</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 注: 对于不理解的步骤, 可以逐行 print 内容查看</span></span><br><span class="line"><span class="comment"># 使用 apply() 方法将 udmap_onethot 函数应用于每个样本的 &#x27;udmap&#x27; 列</span></span><br><span class="line"><span class="comment"># np.vstack() 用于将结果堆叠成一个数组</span></span><br><span class="line">train_udmap_df = pd.DataFrame(np.vstack(train_data[<span class="string">&#x27;udmap&#x27;</span>].apply(udmap_onethot)))</span><br><span class="line">test_udmap_df = pd.DataFrame(np.vstack(test_data[<span class="string">&#x27;udmap&#x27;</span>].apply(udmap_onethot)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">apply() 函数的自由度较高，可以直接对 Series 或者 DataFrame 中元素进行逐元素遍历操作，方便且高效，具有类似于 Numpy 的特性。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为新的特征 DataFrame 命名列名</span></span><br><span class="line">train_udmap_df.columns = [<span class="string">&#x27;key&#x27;</span> + <span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10</span>)]</span><br><span class="line">test_udmap_df.columns = [<span class="string">&#x27;key&#x27;</span> + <span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10</span>)]</span><br><span class="line"><span class="comment"># 将编码后的 udmap 特征与原始数据进行拼接，沿着列方向拼接</span></span><br><span class="line">train_data = pd.concat([train_data, train_udmap_df], axis=<span class="number">1</span>)</span><br><span class="line">test_data = pd.concat([test_data, test_udmap_df], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 编码 udmap 是否为空</span></span><br><span class="line"><span class="comment"># 使用比较运算符将每个样本的 &#x27;udmap&#x27; 列与字符串 &#x27;unknown&#x27; 进行比较，返回一个布尔值的 Series</span></span><br><span class="line"><span class="comment"># 使用 astype(int) 将布尔值转换为整数（0 或 1），以便进行后续的数值计算和分析</span></span><br><span class="line">train_data[<span class="string">&#x27;udmap_isunknown&#x27;</span>] = (train_data[<span class="string">&#x27;udmap&#x27;</span>] == <span class="string">&#x27;unknown&#x27;</span>).astype(<span class="built_in">int</span>)</span><br><span class="line">test_data[<span class="string">&#x27;udmap_isunknown&#x27;</span>] = (test_data[<span class="string">&#x27;udmap&#x27;</span>] == <span class="string">&#x27;unknown&#x27;</span>).astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 提取 eid 的频次特征</span></span><br><span class="line"><span class="comment"># 使用 map() 方法将每个样本的 eid 映射到训练数据中 eid 的频次计数</span></span><br><span class="line"><span class="comment"># train_data[&#x27;eid&#x27;].value_counts() 返回每个 eid 出现的频次计数</span></span><br><span class="line">train_data[<span class="string">&#x27;eid_freq&#x27;</span>] = train_data[<span class="string">&#x27;eid&#x27;</span>].<span class="built_in">map</span>(train_data[<span class="string">&#x27;eid&#x27;</span>].value_counts())</span><br><span class="line">test_data[<span class="string">&#x27;eid_freq&#x27;</span>] = test_data[<span class="string">&#x27;eid&#x27;</span>].<span class="built_in">map</span>(train_data[<span class="string">&#x27;eid&#x27;</span>].value_counts())<span class="comment">#这里在测试数据集上用的是训练集的eid的频率</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; </span></span><br><span class="line"><span class="string">map可以接受函数，字典，以及series（和字典类似）。然后这里会进行匹配。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 提取 eid 的标签特征</span></span><br><span class="line"><span class="comment"># 使用 groupby() 方法按照 eid 进行分组，然后计算每个 eid 分组的目标值均值</span></span><br><span class="line"><span class="comment"># train_data.groupby(&#x27;eid&#x27;)[&#x27;target&#x27;].mean() 返回每个 eid 分组的目标值均值</span></span><br><span class="line">train_data[<span class="string">&#x27;eid_mean&#x27;</span>] = train_data[<span class="string">&#x27;eid&#x27;</span>].<span class="built_in">map</span>(train_data.groupby(<span class="string">&#x27;eid&#x27;</span>)[<span class="string">&#x27;target&#x27;</span>].mean())</span><br><span class="line">test_data[<span class="string">&#x27;eid_mean&#x27;</span>] = test_data[<span class="string">&#x27;eid&#x27;</span>].<span class="built_in">map</span>(train_data.groupby(<span class="string">&#x27;eid&#x27;</span>)[<span class="string">&#x27;target&#x27;</span>].mean())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; </span></span><br><span class="line"><span class="string">这里现根据eid进行分组</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 提取时间戳</span></span><br><span class="line"><span class="comment"># 使用 pd.to_datetime() 函数将时间戳列转换为 datetime 类型</span></span><br><span class="line"><span class="comment"># 样例：1678932546000-&gt;2023-03-15 15:14:16</span></span><br><span class="line"><span class="comment"># 注: 需要注意时间戳的长度, 如果是13位则unit 为 毫秒, 如果是10位则为 秒, 这是转时间戳时容易踩的坑</span></span><br><span class="line"><span class="comment"># 具体实现代码：</span></span><br><span class="line">train_data[<span class="string">&#x27;common_ts&#x27;</span>] = pd.to_datetime(train_data[<span class="string">&#x27;common_ts&#x27;</span>], unit=<span class="string">&#x27;ms&#x27;</span>)</span><br><span class="line">test_data[<span class="string">&#x27;common_ts&#x27;</span>] = pd.to_datetime(test_data[<span class="string">&#x27;common_ts&#x27;</span>], unit=<span class="string">&#x27;ms&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 dt.hour 属性从 datetime 列中提取小时信息，并将提取的小时信息存储在新的列 &#x27;common_ts_hour&#x27;</span></span><br><span class="line">train_data[<span class="string">&#x27;common_ts_hour&#x27;</span>] = train_data[<span class="string">&#x27;common_ts&#x27;</span>].dt.hour</span><br><span class="line">test_data[<span class="string">&#x27;common_ts_hour&#x27;</span>] = test_data[<span class="string">&#x27;common_ts&#x27;</span>].dt.hour</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_data[<span class="string">&#x27;common_ts_day&#x27;</span>] = train_data[<span class="string">&#x27;common_ts&#x27;</span>].dt.day</span><br><span class="line">test_data[<span class="string">&#x27;common_ts_day&#x27;</span>] = test_data[<span class="string">&#x27;common_ts&#x27;</span>].dt.day</span><br><span class="line"></span><br><span class="line">train_data[<span class="string">&#x27;x1_freq&#x27;</span>] = train_data[<span class="string">&#x27;x1&#x27;</span>].<span class="built_in">map</span>(train_data[<span class="string">&#x27;x1&#x27;</span>].value_counts())</span><br><span class="line">test_data[<span class="string">&#x27;x1_freq&#x27;</span>] = test_data[<span class="string">&#x27;x1&#x27;</span>].<span class="built_in">map</span>(train_data[<span class="string">&#x27;x1&#x27;</span>].value_counts())</span><br><span class="line">train_data[<span class="string">&#x27;x1_mean&#x27;</span>] = train_data[<span class="string">&#x27;x1&#x27;</span>].<span class="built_in">map</span>(train_data.groupby(<span class="string">&#x27;x1&#x27;</span>)[<span class="string">&#x27;target&#x27;</span>].mean())</span><br><span class="line">test_data[<span class="string">&#x27;x1_mean&#x27;</span>] = test_data[<span class="string">&#x27;x1&#x27;</span>].<span class="built_in">map</span>(train_data.groupby(<span class="string">&#x27;x1&#x27;</span>)[<span class="string">&#x27;target&#x27;</span>].mean())</span><br><span class="line"></span><br><span class="line">train_data[<span class="string">&#x27;x2_freq&#x27;</span>] = train_data[<span class="string">&#x27;x2&#x27;</span>].<span class="built_in">map</span>(train_data[<span class="string">&#x27;x2&#x27;</span>].value_counts())</span><br><span class="line">test_data[<span class="string">&#x27;x2_freq&#x27;</span>] = test_data[<span class="string">&#x27;x2&#x27;</span>].<span class="built_in">map</span>(train_data[<span class="string">&#x27;x2&#x27;</span>].value_counts())</span><br><span class="line">train_data[<span class="string">&#x27;x2_mean&#x27;</span>] = train_data[<span class="string">&#x27;x2&#x27;</span>].<span class="built_in">map</span>(train_data.groupby(<span class="string">&#x27;x2&#x27;</span>)[<span class="string">&#x27;target&#x27;</span>].mean())</span><br><span class="line">test_data[<span class="string">&#x27;x2_mean&#x27;</span>] = test_data[<span class="string">&#x27;x2&#x27;</span>].<span class="built_in">map</span>(train_data.groupby(<span class="string">&#x27;x2&#x27;</span>)[<span class="string">&#x27;target&#x27;</span>].mean())</span><br><span class="line"></span><br><span class="line"><span class="comment">#train_data[&#x27;x3_freq&#x27;] = train_data[&#x27;x3&#x27;].map(train_data[&#x27;x3&#x27;].value_counts())</span></span><br><span class="line"><span class="comment">#test_data[&#x27;x3_freq&#x27;] = test_data[&#x27;x3&#x27;].map(train_data[&#x27;x3&#x27;].value_counts())</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#train_data[&#x27;x4_freq&#x27;] = train_data[&#x27;x4&#x27;].map(train_data[&#x27;x4&#x27;].value_counts())</span></span><br><span class="line"><span class="comment">#test_data[&#x27;x4_freq&#x27;] = test_data[&#x27;x4&#x27;].map(train_data[&#x27;x4&#x27;].value_counts())</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">这两个数据有问题，在test中会因为数据不匹配导致NaN的出现因此这两个数据剔除</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">train_data[<span class="string">&#x27;x6_freq&#x27;</span>] = train_data[<span class="string">&#x27;x6&#x27;</span>].<span class="built_in">map</span>(train_data[<span class="string">&#x27;x6&#x27;</span>].value_counts())</span><br><span class="line">test_data[<span class="string">&#x27;x6_freq&#x27;</span>] = test_data[<span class="string">&#x27;x6&#x27;</span>].<span class="built_in">map</span>(train_data[<span class="string">&#x27;x6&#x27;</span>].value_counts())</span><br><span class="line">train_data[<span class="string">&#x27;x6_mean&#x27;</span>] = train_data[<span class="string">&#x27;x6&#x27;</span>].<span class="built_in">map</span>(train_data.groupby(<span class="string">&#x27;x6&#x27;</span>)[<span class="string">&#x27;target&#x27;</span>].mean())</span><br><span class="line">test_data[<span class="string">&#x27;x6_mean&#x27;</span>] = test_data[<span class="string">&#x27;x6&#x27;</span>].<span class="built_in">map</span>(train_data.groupby(<span class="string">&#x27;x6&#x27;</span>)[<span class="string">&#x27;target&#x27;</span>].mean())</span><br><span class="line"></span><br><span class="line">train_data[<span class="string">&#x27;x7_freq&#x27;</span>] = train_data[<span class="string">&#x27;x7&#x27;</span>].<span class="built_in">map</span>(train_data[<span class="string">&#x27;x7&#x27;</span>].value_counts())</span><br><span class="line">test_data[<span class="string">&#x27;x7_freq&#x27;</span>] = test_data[<span class="string">&#x27;x7&#x27;</span>].<span class="built_in">map</span>(train_data[<span class="string">&#x27;x7&#x27;</span>].value_counts())</span><br><span class="line">train_data[<span class="string">&#x27;x7_mean&#x27;</span>] = train_data[<span class="string">&#x27;x7&#x27;</span>].<span class="built_in">map</span>(train_data.groupby(<span class="string">&#x27;x7&#x27;</span>)[<span class="string">&#x27;target&#x27;</span>].mean())</span><br><span class="line">test_data[<span class="string">&#x27;x7_mean&#x27;</span>] = test_data[<span class="string">&#x27;x7&#x27;</span>].<span class="built_in">map</span>(train_data.groupby(<span class="string">&#x27;x7&#x27;</span>)[<span class="string">&#x27;target&#x27;</span>].mean())</span><br><span class="line"></span><br><span class="line">train_data[<span class="string">&#x27;x8_freq&#x27;</span>] = train_data[<span class="string">&#x27;x8&#x27;</span>].<span class="built_in">map</span>(train_data[<span class="string">&#x27;x8&#x27;</span>].value_counts())</span><br><span class="line">test_data[<span class="string">&#x27;x8_freq&#x27;</span>] = test_data[<span class="string">&#x27;x8&#x27;</span>].<span class="built_in">map</span>(train_data[<span class="string">&#x27;x8&#x27;</span>].value_counts())</span><br><span class="line">train_data[<span class="string">&#x27;x8_mean&#x27;</span>] = train_data[<span class="string">&#x27;x8&#x27;</span>].<span class="built_in">map</span>(train_data.groupby(<span class="string">&#x27;x8&#x27;</span>)[<span class="string">&#x27;target&#x27;</span>].mean())</span><br><span class="line">test_data[<span class="string">&#x27;x8_mean&#x27;</span>] = test_data[<span class="string">&#x27;x8&#x27;</span>].<span class="built_in">map</span>(train_data.groupby(<span class="string">&#x27;x8&#x27;</span>)[<span class="string">&#x27;target&#x27;</span>].mean())</span><br><span class="line"></span><br><span class="line"><span class="comment">#df.groupby(分组依据)[数据来源].使用操作</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train=train_data.drop([<span class="string">&#x27;udmap&#x27;</span>,<span class="string">&#x27;uuid&#x27;</span>,<span class="string">&#x27;target&#x27;</span>],axis=<span class="number">1</span>)</span><br><span class="line">test=test_data.drop([<span class="string">&#x27;udmap&#x27;</span>,<span class="string">&#x27;uuid&#x27;</span>,],axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#我们保留了common_ts这个数据，接下来对这个特征的归一化</span></span><br></pre></td></tr></table></figure>
<h4 id="数据归一化处理"><a class="markdownIt-Anchor" href="#数据归一化处理"></a> 数据归一化处理</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#对数据进行归一化处理</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> train.columns:</span><br><span class="line">    MAX=<span class="built_in">max</span>(train[i])</span><br><span class="line">    MIN=<span class="built_in">min</span>(train[i])<span class="comment">#用训练集的数据区归一化测试集的数据</span></span><br><span class="line">    LEN=MAX-MIN</span><br><span class="line">    train[i]=train[i].apply(<span class="keyword">lambda</span> x:(x-MIN)/LEN)</span><br><span class="line">    test[i]=test[i].apply(<span class="keyword">lambda</span> x:(x-MIN)/LEN)</span><br></pre></td></tr></table></figure>
<h3 id="特征组合"><a class="markdownIt-Anchor" href="#特征组合"></a> 特征组合</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 暴力Feature 行为</span></span><br><span class="line"><span class="comment"># 暴力Feature 时间</span></span><br><span class="line"><span class="comment"># 暴力Feature 用户属性</span></span><br><span class="line"><span class="comment">#这里暂时不考虑特征的随机组合</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 暴力Feature 行为</span></span><br><span class="line">f = [<span class="string">&#x27;key1&#x27;</span>, <span class="string">&#x27;key2&#x27;</span>, <span class="string">&#x27;key3&#x27;</span>, <span class="string">&#x27;key4&#x27;</span>, <span class="string">&#x27;key5&#x27;</span>, <span class="string">&#x27;key6&#x27;</span>, <span class="string">&#x27;key7&#x27;</span>, <span class="string">&#x27;key8&#x27;</span>, <span class="string">&#x27;key9&#x27;</span>,<span class="string">&#x27;udmap_isunknown&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> df <span class="keyword">in</span> [train, test]:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(f)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, <span class="built_in">len</span>(f)):</span><br><span class="line"><span class="comment">#加f后可以在字符串里面使用用花括号括起来的变量和表达式，如果字符串里面没有表达式，那么前面加不加f输出应该都一样。</span></span><br><span class="line">            df[<span class="string">f&#x27;<span class="subst">&#123;f[i]&#125;</span>+<span class="subst">&#123;f[j]&#125;</span>&#x27;</span>] = df[f[i]] + df[f[j]]</span><br><span class="line"><span class="comment"># 暴力Feature 时间</span></span><br><span class="line">f = [<span class="string">&#x27;common_ts_hour&#x27;</span>,<span class="string">&#x27;common_ts_day&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> df <span class="keyword">in</span> [train, test]:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(f)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, <span class="built_in">len</span>(f)):</span><br><span class="line">            df[<span class="string">f&#x27;<span class="subst">&#123;f[i]&#125;</span>+<span class="subst">&#123;f[j]&#125;</span>&#x27;</span>] = df[f[i]] + df[f[j]]</span><br><span class="line">            df[<span class="string">f&#x27;<span class="subst">&#123;f[i]&#125;</span>-<span class="subst">&#123;f[j]&#125;</span>&#x27;</span>] = df[f[i]] - df[f[j]]</span><br><span class="line">            df[<span class="string">f&#x27;<span class="subst">&#123;f[i]&#125;</span>*<span class="subst">&#123;f[j]&#125;</span>&#x27;</span>] = df[f[i]] * df[f[j]]</span><br><span class="line">            df[<span class="string">f&#x27;<span class="subst">&#123;f[i]&#125;</span>/<span class="subst">&#123;f[j]&#125;</span>&#x27;</span>] = df[f[i]] / (df[f[j]]+<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 暴力Feature 用户属性</span></span><br><span class="line">f = [<span class="string">&#x27;x1&#x27;</span>, <span class="string">&#x27;x2&#x27;</span>, <span class="string">&#x27;x3&#x27;</span>, <span class="string">&#x27;x4&#x27;</span>, <span class="string">&#x27;x5&#x27;</span>, <span class="string">&#x27;x6&#x27;</span>, <span class="string">&#x27;x7&#x27;</span>, <span class="string">&#x27;x8&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> df <span class="keyword">in</span> [train, test]:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(f)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, <span class="built_in">len</span>(f)):</span><br><span class="line">            df[<span class="string">f&#x27;<span class="subst">&#123;f[i]&#125;</span>+<span class="subst">&#123;f[j]&#125;</span>&#x27;</span>] = df[f[i]] + df[f[j]]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="数据降维"><a class="markdownIt-Anchor" href="#数据降维"></a> 数据降维</h3>
<ul>
<li>利用xgboost进行特征选择，最终选出70组特征</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#采用xgboost的特征筛选的功能</span></span><br><span class="line">xgbc = XGBClassifier(</span><br><span class="line">    objective=<span class="string">&#x27;binary:logistic&#x27;</span>,</span><br><span class="line">    eval_metric=<span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">    n_estimators=<span class="number">100</span>, </span><br><span class="line">    max_depth=<span class="number">6</span>, </span><br><span class="line">    learning_rate=<span class="number">0.1</span></span><br><span class="line">)</span><br><span class="line">xgbc.fit(train, label)</span><br><span class="line">importances_xgb = xgbc.feature_importances_/np.<span class="built_in">sum</span>( xgbc.feature_importances_)</span><br><span class="line"><span class="comment"># print(importances)</span></span><br><span class="line">indices_xgb = np.argsort(importances_xgb)[::-<span class="number">1</span>]</span><br><span class="line"><span class="comment"># print(indices)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看结果</span></span><br><span class="line">feat_labels = train.columns</span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> <span class="built_in">range</span>(train.shape[<span class="number">1</span>]):  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%2d) %-*s %f&quot;</span> % \</span><br><span class="line">          (f + <span class="number">1</span>, <span class="number">30</span>, feat_labels[indices_xgb[f]], importances_xgb[indices_xgb[f]]))</span><br><span class="line"></span><br><span class="line">features=np.array(feat_labels)</span><br><span class="line">num_imo=features[<span class="built_in">list</span>(indices_xgb[<span class="number">0</span>:<span class="number">60</span>])]<span class="comment">#选择60个特征</span></span><br><span class="line"></span><br><span class="line">train=train[num_imo]</span><br><span class="line">test=test[num_imo]</span><br></pre></td></tr></table></figure>
<h3 id="交叉验证模型"><a class="markdownIt-Anchor" href="#交叉验证模型"></a> 交叉验证模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 常见的交叉验证模型框架</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model_train</span>(<span class="params">model, model_name, kfold=<span class="number">5</span></span>):</span><br><span class="line">    oof_preds = np.zeros((train.shape[<span class="number">0</span>]))<span class="comment">#构造一个series令所有行全部为0</span></span><br><span class="line">    test_preds = np.zeros(test.shape[<span class="number">0</span>])</span><br><span class="line">    skf = StratifiedKFold(n_splits=kfold)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Model = <span class="subst">&#123;model_name&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(train.columns))</span><br><span class="line">    <span class="keyword">for</span> k, (train_index, test_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(skf.split(train, label)):</span><br><span class="line">        x_train, x_test = train.iloc[train_index, :], train.iloc[test_index, :]</span><br><span class="line">        y_train, y_test = label.iloc[train_index], label.iloc[test_index]</span><br><span class="line">        <span class="built_in">print</span>(<span class="number">1</span>)</span><br><span class="line">        model.fit(x_train,y_train)</span><br><span class="line">        <span class="comment">#print(2)</span></span><br><span class="line">        y_pred = model.predict_proba(x_test)[:,<span class="number">1</span>]</span><br><span class="line">        <span class="comment">##在这里第一列是预测为0的概率，第二列是预测为1的概率</span></span><br><span class="line">        oof_preds[test_index] = y_pred.ravel()</span><br><span class="line">        auc = roc_auc_score(y_test,y_pred)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;- KFold = %d, val_auc = %.4f&quot;</span> % (k, auc))</span><br><span class="line">        test_fold_preds = model.predict_proba(test)[:, <span class="number">1</span>]</span><br><span class="line">        test_preds += test_fold_preds.ravel()<span class="comment">#将给定Series对象的基础数据作为ndarray返回。</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Overall Model = %s, F1 = %.4f&quot;</span> % (model_name, f1_score(label, oof_preds, average=<span class="string">&#x27;macro&#x27;</span>)))</span><br><span class="line">    <span class="keyword">return</span> test_preds / kfold<span class="comment">#取平均值</span></span><br></pre></td></tr></table></figure>
<h3 id="数据清洗通过10交叉验证判断数据是否存在问题"><a class="markdownIt-Anchor" href="#数据清洗通过10交叉验证判断数据是否存在问题"></a> 数据清洗，通过10交叉验证判断数据是否存在问题</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">xgbc = XGBClassifier(</span></span><br><span class="line"><span class="string">    objective=&#x27;binary:logistic&#x27;,</span></span><br><span class="line"><span class="string">    eval_metric=&#x27;auc&#x27;,</span></span><br><span class="line"><span class="string">    n_estimators=100, </span></span><br><span class="line"><span class="string">    max_depth=6, </span></span><br><span class="line"><span class="string">    learning_rate=0.1</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">xgbc_test_preds = model_train(xgbc, &quot;XGBClassifier&quot;, 10)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#这里用于挑选异常训练集</span></span><br><span class="line"><span class="comment">#看误差是否过大</span></span><br></pre></td></tr></table></figure>
<h3 id="验证集和训练集构建"><a class="markdownIt-Anchor" href="#验证集和训练集构建"></a> 验证集和训练集构建</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#先将训练数据划分成训练集和验证集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split( train, label, stratify=label, random_state=<span class="number">2022</span>)</span><br><span class="line"><span class="comment">#75%的训练集</span></span><br></pre></td></tr></table></figure>
<h3 id="模型选择"><a class="markdownIt-Anchor" href="#模型选择"></a> 模型选择</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># xgboost实验</span></span><br><span class="line"><span class="comment"># max_depth不能太小否则会出问题</span></span><br><span class="line">xgbc = XGBClassifier(</span><br><span class="line">    objective=<span class="string">&#x27;binary:logistic&#x27;</span>,</span><br><span class="line">    eval_metric=<span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">    n_estimators=<span class="number">100</span>, </span><br><span class="line">    max_depth=<span class="number">50</span>, </span><br><span class="line">    learning_rate=<span class="number">0.1</span></span><br><span class="line">)</span><br><span class="line">xgbc.fit(x_train,y_train)</span><br><span class="line">y_pred = xgbc.predict_proba(x_test)[:, <span class="number">1</span>]</span><br><span class="line">threshold=<span class="number">0.5</span></span><br><span class="line">y_pred = (y_pred &gt;= threshold).astype(<span class="built_in">int</span>)</span><br><span class="line">f1 = f1_score(y_test, y_pred, average=<span class="string">&#x27;macro&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;F1 = %.8f&#x27;</span> % f1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 决策树实验</span></span><br><span class="line">DT = DecisionTreeClassifier()</span><br><span class="line">DT.fit(x_train,y_train)</span><br><span class="line">y_pred = DT.predict_proba(x_test)[:, <span class="number">1</span>]</span><br><span class="line">f1 = f1_score(y_test, y_pred, average=<span class="string">&#x27;macro&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;F1 = %.8f&#x27;</span> % f1)</span><br><span class="line"></span><br><span class="line"><span class="comment">#随机森林实验</span></span><br><span class="line">RF=RandomForestClassifier(n_estimators=<span class="number">50</span>)</span><br><span class="line">RF.fit(x_train,y_train)</span><br><span class="line">y_pred = RF.predict_proba(x_test)[:, <span class="number">1</span>]</span><br><span class="line">threshold=<span class="number">0.5</span></span><br><span class="line">y_pred = (y_pred &gt;= threshold).astype(<span class="built_in">int</span>)</span><br><span class="line">f1 = f1_score(y_test, y_pred, average=<span class="string">&#x27;macro&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;F1 = %.8f&#x27;</span> % f1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># GDBT实验</span></span><br><span class="line"><span class="comment">#是不是树的深度太浅导致的</span></span><br><span class="line">gbc = GradientBoostingClassifier(</span><br><span class="line">    n_estimators=<span class="number">10</span>, </span><br><span class="line">    learning_rate=<span class="number">0.1</span>,</span><br><span class="line">    max_depth=<span class="number">50</span></span><br><span class="line">)</span><br><span class="line">gbc.fit(x_train,y_train)</span><br><span class="line">y_pred = gbc.predict_proba(x_test)[:, <span class="number">1</span>]</span><br><span class="line">threshold=<span class="number">0.5</span></span><br><span class="line">y_pred = (y_pred &gt;= threshold).astype(<span class="built_in">int</span>)</span><br><span class="line">f1 = f1_score(y_test, y_pred, average=<span class="string">&#x27;macro&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;F1 = %.8f&#x27;</span> % f1)</span><br><span class="line"></span><br><span class="line"><span class="comment">#HGBC 实验</span></span><br><span class="line">hgbc = HistGradientBoostingClassifier(</span><br><span class="line">    max_iter=<span class="number">20</span>,</span><br><span class="line">    max_depth=<span class="number">50</span></span><br><span class="line">)</span><br><span class="line">gbc.fit(x_train,y_train)</span><br><span class="line">y_pred = gbc.predict_proba(x_test)[:, <span class="number">1</span>]</span><br><span class="line">threshold=<span class="number">0.5</span></span><br><span class="line">y_pred = (y_pred &gt;= threshold).astype(<span class="built_in">int</span>)</span><br><span class="line">f1 = f1_score(y_test, y_pred, average=<span class="string">&#x27;macro&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;F1 = %.8f&#x27;</span> % f1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># #LGMB 实验</span></span><br><span class="line"><span class="comment"># gbm = LGBMClassifier(</span></span><br><span class="line"><span class="comment">#     objective=&#x27;binary&#x27;,</span></span><br><span class="line"><span class="comment">#     boosting_type=&#x27;gbdt,</span></span><br><span class="line"><span class="comment">#     num_leaves=2 ** 6, </span></span><br><span class="line"><span class="comment">#     max_depth=50,</span></span><br><span class="line"><span class="comment">#     colsample_bytree=0.8,</span></span><br><span class="line"><span class="comment">#     subsample_freq=1,</span></span><br><span class="line"><span class="comment">#     max_bin=255,</span></span><br><span class="line"><span class="comment">#     learning_rate=0.05, </span></span><br><span class="line"><span class="comment">#     n_estimators=4000, </span></span><br><span class="line"><span class="comment">#     metrics=&#x27;auc&#x27;</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"><span class="comment"># gbm.fit(x_train,y_train)</span></span><br><span class="line"><span class="comment"># y_pred = gbm.predict_proba(x_train)[:, 1]</span></span><br><span class="line"><span class="comment"># threshold=0.5</span></span><br><span class="line"><span class="comment"># y_pred = (y_pred &gt;= threshold).astype(int)</span></span><br><span class="line"><span class="comment"># f1 = f1_score(y_train, y_pred, average=&#x27;macro&#x27;)</span></span><br><span class="line"><span class="comment"># print(&#x27;F1 = %.8f&#x27; % f1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cbc = CatBoostClassifier(</span></span><br><span class="line"><span class="comment">#     iterations=20, </span></span><br><span class="line"><span class="comment">#     depth=16, </span></span><br><span class="line"><span class="comment">#     learning_rate=0.03, </span></span><br><span class="line"><span class="comment">#     l2_leaf_reg=1, </span></span><br><span class="line"><span class="comment">#     loss_function=&#x27;Logloss&#x27;, </span></span><br><span class="line"><span class="comment">#     verbose=0</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"><span class="comment"># cbc.fit(x_train,y_train)</span></span><br><span class="line"><span class="comment"># y_pred = cbc.predict_proba(x_train)[:, 1]</span></span><br><span class="line"><span class="comment"># threshold=0.5</span></span><br><span class="line"><span class="comment"># y_pred = (y_pred &gt;= threshold).astype(int)</span></span><br><span class="line"><span class="comment"># f1 = f1_score(y_train, y_pred, average=&#x27;macro&#x27;)</span></span><br><span class="line"><span class="comment"># print(&#x27;F1 = %.8f&#x27; % f1)</span></span><br><span class="line"></span><br><span class="line">ada=AdaBoostClassifier(</span><br><span class="line">    DecisionTreeClassifier(max_depth=<span class="number">50</span>),</span><br><span class="line">    n_estimators=<span class="number">100</span>,</span><br><span class="line">    learning_rate=<span class="number">0.01</span></span><br><span class="line">    )<span class="comment">#默认是CART决策树作为单模型</span></span><br><span class="line">ada.fit(x_train,y_train)</span><br><span class="line">y_pred = ada.predict_proba(x_test)[:, <span class="number">1</span>]</span><br><span class="line">threshold=<span class="number">0.5</span></span><br><span class="line">y_pred = (y_pred &gt;= threshold).astype(<span class="built_in">int</span>)</span><br><span class="line">f1 = f1_score(y_test, y_pred, average=<span class="string">&#x27;macro&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;F1 = %.8f&#x27;</span> % f1)</span><br></pre></td></tr></table></figure>
<h3 id="模型融合"><a class="markdownIt-Anchor" href="#模型融合"></a> 模型融合</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 最终决定：决策树，xgboost， RF，GBDT，HGBC,adaboost这几个模型stack</span></span><br><span class="line">xgbc = XGBClassifier(</span><br><span class="line">    objective=<span class="string">&#x27;binary:logistic&#x27;</span>,</span><br><span class="line">    eval_metric=<span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">    n_estimators=<span class="number">100</span>, </span><br><span class="line">    max_depth=<span class="number">50</span>, </span><br><span class="line">    learning_rate=<span class="number">0.1</span></span><br><span class="line">)</span><br><span class="line">DT = DecisionTreeClassifier()</span><br><span class="line">RF=RandomForestClassifier(n_estimators=<span class="number">50</span>)</span><br><span class="line">gbc = GradientBoostingClassifier(</span><br><span class="line">    n_estimators=<span class="number">10</span>, </span><br><span class="line">    learning_rate=<span class="number">0.1</span>,</span><br><span class="line">    max_depth=<span class="number">50</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">hgbc = HistGradientBoostingClassifier(</span><br><span class="line">    max_iter=<span class="number">20</span>,</span><br><span class="line">    max_depth=<span class="number">50</span></span><br><span class="line">)</span><br><span class="line">ada=AdaBoostClassifier(</span><br><span class="line">    DecisionTreeClassifier(max_depth=<span class="number">50</span>),</span><br><span class="line">    n_estimators=<span class="number">100</span>,</span><br><span class="line">    learning_rate=<span class="number">0.01</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">estimators = [</span><br><span class="line">    (<span class="string">&#x27;xgbc&#x27;</span>, xgbc),</span><br><span class="line">    (<span class="string">&#x27;DT&#x27;</span>,DT),</span><br><span class="line">    (<span class="string">&#x27;RF&#x27;</span>,RF),</span><br><span class="line">    (<span class="string">&#x27;gbc&#x27;</span>, gbc),</span><br><span class="line">    (<span class="string">&#x27;hgbc&#x27;</span>, hgbc),</span><br><span class="line">    (<span class="string">&#x27;ada&#x27;</span>, ada),</span><br><span class="line">]</span><br><span class="line">clf = StackingClassifier(</span><br><span class="line">    estimators=estimators, </span><br><span class="line">    final_estimator=LogisticRegression()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#用组合模型训练</span></span><br><span class="line">clf.fit(x_train, y_train)</span><br><span class="line">y_pred = clf.predict_proba(x_test)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">threshold=<span class="number">0.5</span></span><br><span class="line">y_pred = (y_pred &gt;= threshold).astype(<span class="built_in">int</span>)</span><br><span class="line">f1 = f1_score(y_test, y_pred, average=<span class="string">&#x27;macro&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;F1 = %.8f&#x27;</span> % f1)</span><br></pre></td></tr></table></figure>
<h3 id="结果提交"><a class="markdownIt-Anchor" href="#结果提交"></a> 结果提交</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># #这里的分类器我不单单想用上面的，我打算重新训练所有数据集来进行预测</span></span><br><span class="line"><span class="comment"># clf_test_preds = model_train(clf, &quot;StackingClassifier&quot;)</span></span><br><span class="line"><span class="comment"># #还是用全部的数据进行训练 </span></span><br><span class="line"><span class="comment"># clf.fit(train,label)</span></span><br><span class="line"></span><br><span class="line">result_df = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;uuid&#x27;</span>: test_data[<span class="string">&#x27;uuid&#x27;</span>],  <span class="comment"># 使用测试数据集中的 &#x27;uuid&#x27; 列作为 &#x27;uuid&#x27; 列的值</span></span><br><span class="line">    <span class="string">&#x27;target&#x27;</span>: clf.predict(test)  <span class="comment"># 使用模型 clf 对测试数据集进行预测，并将预测结果存储在 &#x27;target&#x27; 列中</span></span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">result_df.to_csv(<span class="string">&#x27;submit.csv&#x27;</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>竞赛</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch_torch.autograd.Function</title>
    <url>/2023/08/24/pytorch-torch-autograd-Function/</url>
    <content><![CDATA[<h1 id="这是关于torchautogradfunction"><a class="markdownIt-Anchor" href="#这是关于torchautogradfunction"></a> 这是关于torch.autograd.Function</h1>
<p>在 PyTorch 中，<code>torch.autograd.Function</code> 是一个基础类，用于定义自定义的autograd函数，使你能够实现任意的前向传播和反向传播操作。这对于实现自定义的操作和损失函数，或者对已有操作进行修改，都非常有用。</p>
<span id="more"></span>
<p>要使用 <code>torch.autograd.Function</code>，你需要创建一个继承自它的子类，并实现以下两个方法：<code>forward</code> 和 <code>backward</code>。</p>
<ol>
<li>
<p><code>forward</code> 方法：<br>
这个方法定义了自定义函数的前向传播过程。它接收输入张量或其他变量作为参数，并返回计算结果。在 <code>forward</code> 方法中，你可以执行任意计算，包括创建新的张量和执行运算符。</p>
</li>
<li>
<p><code>backward</code> 方法：<br>
这个方法定义了自定义函数的反向传播过程。它接收关于输出的梯度（通常是一个梯度张量）作为参数，并计算相对于输入的梯度。在 <code>backward</code> 方法中，你需要计算输入变量的梯度，以便在整个计算图中进行梯度传播。</p>
</li>
</ol>
<p>以下是一个简单的示例，演示如何使用 <code>torch.autograd.Function</code> 来实现一个自定义函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyFunction</span>(torch.autograd.Function):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># 在 forward 方法中执行前向传播计算</span></span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>)</span><br><span class="line">        output = <span class="built_in">input</span> * <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">        <span class="comment"># 在 backward 方法中计算梯度</span></span><br><span class="line">        <span class="built_in">input</span>, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output * <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用自定义函数</span></span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = MyFunction.apply(x)</span><br><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Input gradient:&quot;</span>, x.grad)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Input gradient: tensor([2.])</span></span><br><span class="line"><span class="string">tensor([2.], grad_fn=&lt;MyFunctionBackward&gt;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>在这个示例中，<code>MyFunction</code> 继承自 <code>torch.autograd.Function</code>，并实现了 <code>forward</code> 和 <code>backward</code> 方法。你可以通过 <code>MyFunction.apply()</code> 来使用这个自定义函数。在后续的反向传播中，PyTorch 将会使用 <code>backward</code> 方法计算梯度。</p>
<p>这就是如何使用 <code>torch.autograd.Function</code> 来实现自定义函数，并在自定义的计算中使用 PyTorch 的自动微分。</p>
<ul>
<li>
<p><code>@staticmethod</code> 是 Python 中的一个装饰器（Decorator），用于将一个方法定义为静态方法。静态方法是指在类中定义的方法，不依赖于类的实例，因此可以直接通过类名调用，而不需要创建类的对象实例。</p>
<p>在你提供的代码中，<code>@staticmethod</code> 装饰器用于将方法定义为静态方法。具体来说，它用于 <code>SpecialSpmmFunction</code> 类中的两个方法：<code>forward</code> 和 <code>backward</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SpecialSpmmFunction</span>(torch.autograd.Function):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, indices, values, shape, b</span>):</span><br><span class="line">        <span class="comment"># ... implementation ...</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">        <span class="comment"># ... implementation ...</span></span><br></pre></td></tr></table></figure>
<p>通过将这两个方法定义为静态方法，你可以在不创建类的实例的情况下，直接通过类名调用这些方法。例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">indices = ...</span><br><span class="line">values = ...</span><br><span class="line">shape = ...</span><br><span class="line">b = ...</span><br><span class="line">result = SpecialSpmmFunction.forward(indices, values, shape, b)</span><br></pre></td></tr></table></figure>
<p>这种方法非常适合在定义类的方法时，不需要访问实例属性或方法，或者在类的实例不存在的情况下执行一些操作。静态方法不会自动接收类的实例作为第一个参数（通常是 <code>self</code>），因此它们不依赖于类的状态。</p>
</li>
<li>
<p>在上面的代码中，<code>y = MyFunction.apply(x)</code> 这一行代码是通过调用 <code>MyFunction</code> 类的 <code>apply</code> 方法来计算前向传播的结果 <code>y</code>。在这个特定的示例中，<code>MyFunction</code> 类的 <code>forward</code> 方法执行的操作是将输入张量 <code>x</code> 乘以 2，因此 <code>y</code> 的值将是 <code>x</code> 的两倍。</p>
<p>这里，<code>MyFunction.apply(x)</code> 实际上是在前向传播中使用了自定义的操作，并返回计算得到的输出。因为我们定义了自定义函数 <code>MyFunction</code> 的 <code>forward</code> 方法，所以调用 <code>.apply(x)</code> 实际上就是调用了我们自己实现的操作。</p>
<p>在更复杂的情况下，自定义函数可能会执行许多不同的操作，从而实现复杂的前向传播。<code>apply</code> 方法允许我们将输入传递给这些操作，并返回输出。通常情况下，PyTorch 的模块和函数也是这样工作的，只是在内部使用了更多的优化和组件。</p>
<p>简而言之，<code>y = MyFunction.apply(x)</code> 将会调用自定义函数 <code>MyFunction</code> 的前向传播方法，执行该方法中的操作，并将操作的结果存储在 <code>y</code> 中。</p>
</li>
<li>
<p>对于print(y)</p>
<ul>
<li>
<p>在上面的代码中，<code>y = MyFunction.apply(x)</code> 这一行代码是通过调用 <code>MyFunction</code> 类的 <code>apply</code> 方法来计算<strong>前向传播的结果</strong> <code>y</code>。在这个特定的示例中，<code>MyFunction</code> 类的 <code>forward</code> 方法执行的操作是将输入张量 <code>x</code> 乘以 2，因此 <code>y</code> 的值将是 <code>x</code> 的两倍。</p>
<p>这里，<code>MyFunction.apply(x)</code> 实际上是在前向传播中使用了自定义的操作，并返回计算得到的输出。因为我们定义了自定义函数 <code>MyFunction</code> 的 <code>forward</code> 方法，所以调用 <code>.apply(x)</code> 实际上就是调用了我们自己实现的操作。</p>
<p>在更复杂的情况下，自定义函数可能会执行许多不同的操作，从而实现复杂的前向传播。<code>apply</code> 方法允许我们将输入传递给这些操作，并返回输出。通常情况下，PyTorch 的模块和函数也是这样工作的，只是在内部使用了更多的优化和组件。</p>
<p>简而言之，<code>y = MyFunction.apply(x)</code> 将会调用自定义函数 <code>MyFunction</code> 的前向传播方法，执行该方法中的操作，并将操作的结果存储在 <code>y</code> 中。</p>
</li>
</ul>
</li>
<li>
<p>如果令c=y.backward(),print©输出的结果为None</p>
</li>
<li>
<p>如果将y.backward()注释掉，print(“Input gradient:”, x.grad)为Input gradient:None</p>
</li>
</ul>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>计算机语言</tag>
      </tags>
  </entry>
</search>
